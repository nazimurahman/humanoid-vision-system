# configs/training.yaml
# Training-specific configuration

# Training Process
training:
  mode: "supervised"  # supervised, semi_supervised, self_supervised
  batch_size: 16
  num_epochs: 100
  warmup_epochs: 10
  validation_frequency: 1  # Validate every N epochs
  save_frequency: 5  # Save checkpoint every N epochs
  
# Dataset Configuration
dataset:
  name: "coco2017"  # coco2017, custom, synthetic
  train_split: "train2017"
  val_split: "val2017"
  test_split: "test2017"
  
  # Paths (override in deployment)
  train_images: "${paths.data_dir}/coco/train2017"
  train_annotations: "${paths.data_dir}/coco/annotations/instances_train2017.json"
  val_images: "${paths.data_dir}/coco/val2017"
  val_annotations: "${paths.data_dir}/coco/annotations/instances_val2017.json"
  
  # Class mapping
  class_names: [
    "person", "bicycle", "car", "motorcycle", "airplane", "bus", "train", "truck",
    "boat", "traffic light", "fire hydrant", "stop sign", "parking meter", "bench",
    "bird", "cat", "dog", "horse", "sheep", "cow", "elephant", "bear", "zebra",
    "giraffe", "backpack", "umbrella", "handbag", "tie", "suitcase", "frisbee",
    "skis", "snowboard", "sports ball", "kite", "baseball bat", "baseball glove",
    "skateboard", "surfboard", "tennis racket", "bottle", "wine glass", "cup",
    "fork", "knife", "spoon", "bowl", "banana", "apple", "sandwich", "orange",
    "broccoli", "carrot", "hot dog", "pizza", "donut", "cake", "chair", "couch",
    "potted plant", "bed", "dining table", "toilet", "tv", "laptop", "mouse",
    "remote", "keyboard", "cell phone", "microwave", "oven", "toaster", "sink",
    "refrigerator", "book", "clock", "vase", "scissors", "teddy bear", "hair drier",
    "toothbrush"
  ]
  
# Advanced Augmentation for Training
augmentation:
  advanced:
    mosaic:
      enabled: true
      probability: 0.5
      mixup_ratio: 0.2
      
    mixup:
      enabled: true
      alpha: 0.2
      
    cutmix:
      enabled: true
      alpha: 1.0
      
    random_affine:
      enabled: true
      degrees: 10
      translate: 0.1
      scale: [0.5, 1.5]
      shear: 5
      
    blur:
      enabled: true
      probability: 0.1
      kernel_size: [3, 5, 7]
      
    noise:
      enabled: true
      probability: 0.1
      gaussian_std: [0.01, 0.03]
      
# Optimizer Configuration
optimizer:
  type: "AdamW"  # AdamW, SGD, Adam, RMSprop
  learning_rate: 0.001
  weight_decay: 0.0001
  momentum: 0.9  # For SGD
  beta1: 0.9    # For Adam/AdamW
  beta2: 0.999  # For Adam/AdamW
  epsilon: 1e-8
  
  # Gradient Clipping (mHC-specific)
  gradient_clipping:
    enabled: true
    clip_type: "norm"  # norm, value
    max_norm: 1.0
    mhc_max_norm: 0.5  # Tighter for MHC layers
    clip_value: 1.0
    
# Learning Rate Scheduler
scheduler:
  type: "CosineAnnealingWarmRestarts"  # Cosine, Step, ReduceLROnPlateau, OneCycle
  warmup_epochs: 10
  min_lr: 1e-6
  T_0: 10  # Initial restart period
  T_mult: 2  # Multiplier for each restart
  
  # For StepLR
  step_size: 30
  gamma: 0.1
  
  # For ReduceLROnPlateau
  factor: 0.1
  patience: 10
  threshold: 1e-4
  
# Loss Functions
loss:
  detection:
    type: "yolo"  # yolo, focal, giou
    weights:
      box: 5.0
      obj: 1.0
      cls: 1.0
      noobj: 0.5
      
  classification:
    type: "cross_entropy"  # cross_entropy, label_smoothing
    label_smoothing: 0.1
    
# Early Stopping
early_stopping:
  enabled: true
  patience: 20
  min_delta: 0.001
  restore_best_weights: true
  
# Mixed Precision Training
mixed_precision:
  enabled: true
  dtype: "bfloat16"  # bfloat16, float16
  scale_factor: 65536.0
  growth_factor: 2.0
  backoff_factor: 0.5
  growth_interval: 2000
  
# Gradient Accumulation
gradient_accumulation:
  enabled: false
  steps: 4  # Effective batch size = batch_size * steps
  
# Distributed Training
distributed:
  enabled: false
  backend: "nccl"  # nccl, gloo
  init_method: "env://"
  world_size: 1
  rank: 0
  
# Hyperparameter Search (Optional)
hyperparameter_search:
  enabled: false
  method: "random"  # random, grid, bayesian
  num_trials: 50
  search_space:
    learning_rate:
      min: 1e-5
      max: 1e-2
      scale: "log"
    batch_size: [8, 16, 32]
    
# Training Monitoring
monitoring:
  log_frequency: 100  # Steps
  validation_frequency: 1000  # Steps
  checkpoint_frequency: 5000  # Steps
  
  metrics_to_track:
    - "loss/total"
    - "loss/box"
    - "loss/obj"
    - "loss/cls"
    - "mAP@0.5"
    - "mAP@0.5:0.95"
    - "precision"
    - "recall"
    - "learning_rate"
    - "grad_norm"
    
  # MHC-specific stability metrics
  stability_metrics:
    - "mhc/max_eigenvalue"
    - "mhc/min_eigenvalue"
    - "mhc/signal_ratio"
    - "mhc/gradient_norms"
    - "sinkhorn/convergence"
    
# Data Parallelism
data_parallel:
  enabled: false
  sync_batchnorm: true
  find_unused_parameters: false
  
# Model Parallelism
model_parallel:
  enabled: false
  pipeline_parallel_degree: 1
  tensor_parallel_degree: 1
  
# Checkpoint Management
checkpoint:
  resume_from: null  # Path to checkpoint to resume from
  load_optimizer: true
  load_scheduler: true
  strict_loading: true
  
# Debugging and Profiling
debug:
  profile: false
  profile_steps: [10, 20]  # Steps to profile
  memory_snapshot: false
  gradient_flow: false
  nan_detection: true
  
# Experiment Tracking
experiment:
  name: "baseline_training"
  tags: ["hybrid", "mhc", "coco"]
  notes: "Baseline training with MHC constraints"
  
# Output Configuration
output:
  model_format: "torchscript"  # torchscript, onnx, tensorrt
  export_quantized: false
  include_metadata: true
  metadata:
    author: "Humanoid Vision Team"
    description: "Hybrid vision system with manifold constraints"
    license: "Apache 2.0"
    version: "${system.version}"