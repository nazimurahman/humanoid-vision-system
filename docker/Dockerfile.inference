# docker/Dockerfile.inference
# Optimized inference container for production

# Stage 1: Minimal base
FROM nvidia/cuda:12.1.0-runtime-ubuntu22.04 AS base

# Install minimal dependencies
RUN apt-get update && apt-get install -y \
    python3.10 \
    python3.10-venv \
    libgl1-mesa-glx \
    libglib2.0-0 \
    libsm6 \
    libxext6 \
    libgomp1 \
    && rm -rf /var/lib/apt/lists/* \
    && ln -s /usr/bin/python3.10 /usr/bin/python

# Create non-root user
RUN useradd -m -u 1001 -s /bin/bash inference
USER inference
WORKDIR /app

# Create virtual environment
RUN python3.10 -m venv /app/venv
ENV PATH="/app/venv/bin:$PATH"

# Stage 2: Dependencies
FROM base AS dependencies

# Copy requirements
COPY --chown=inference:inference requirements.txt /app/
COPY --chown=inference:inference requirements-inference.txt /app/

# Install dependencies
RUN pip install --upgrade pip setuptools wheel && \
    pip install --no-cache-dir -r /app/requirements.txt && \
    pip install --no-cache-dir -r /app/requirements-inference.txt && \
    pip install --no-cache-dir \
    torch==2.1.0 \
    torchvision==0.16.0 \
    --extra-index-url https://download.pytorch.org/whl/cu121

# Install optimized inference packages
RUN pip install --no-cache-dir \
    onnxruntime-gpu==1.16.0 \
    tensorrt==8.6.1 \
    pycuda==2023.1 \
    fastapi==0.104.1 \
    uvicorn==0.24.0 \
    grpcio==1.59.3 \
    grpcio-tools==1.59.3 \
    prometheus-client==0.19.0

# Stage 3: Production image
FROM dependencies AS production

# Copy application code
COPY --chown=inference:inference src/ /app/src/
COPY --chown=inference:inference configs/ /app/configs/
COPY --chown=inference:inference scripts/ /app/scripts/
COPY --chown=inference:inference entrypoint.sh /app/

# Copy models (usually mounted, but include default)
RUN mkdir -p /app/models
COPY --chown=inference:inference models/pretrained/vision_model.pt /app/models/

# Set environment variables
ENV CUDA_VISIBLE_DEVICES=0 \
    OMP_NUM_THREADS=1 \
    MODEL_PATH="/app/models/vision_model.pt" \
    CONFIG_PATH="/app/configs/inference.yaml" \
    LOG_LEVEL="INFO" \
    PORT=8000 \
    GRPC_PORT=50051 \
    WORKERS=1 \
    MAX_BATCH_SIZE=8

# Create necessary directories
RUN mkdir -p /app/logs /app/tmp

# Expose ports
# HTTP API
EXPOSE 8000  
# gRPC
EXPOSE 50051
# Prometheus metrics
EXPOSE 9090  

# Health check
HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# Set entrypoint
ENTRYPOINT ["/app/entrypoint.sh"]

# Default command
CMD ["api"]