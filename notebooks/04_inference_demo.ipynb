{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70027e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "\"\"\"\n",
    "# Inference Demo and Performance Analysis\n",
    "## Humanoid Vision System - Real-time Inference\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. Model loading and preparation\n",
    "2. Real-time inference pipeline\n",
    "3. Performance benchmarking\n",
    "4. Visualization and analysis\n",
    "5. Deployment testing\n",
    "\"\"\"\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 1. Setup and Imports\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Any, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "\n",
    "# Inference modules\n",
    "from src.inference.engine import InferenceEngine\n",
    "from src.inference.preprocessing import ImagePreprocessor\n",
    "from src.inference.postprocessing import DetectionPostprocessor\n",
    "from src.inference.visualizer import DetectionVisualizer\n",
    "from src.inference.robot_interface import RobotCommunicationInterface\n",
    "\n",
    "# Model\n",
    "from src.models.hybrid_vision import HybridVisionSystem\n",
    "from src.utils.logging import setup_logger\n",
    "\n",
    "# Visualization\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Performance monitoring\n",
    "import psutil\n",
    "import GPUtil\n",
    "from memory_profiler import memory_usage\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Configuration\n",
    "config = {\n",
    "    'model': {\n",
    "        'path': '../models/checkpoints/model_epoch_050_loss_1.234.pt',  # Example path\n",
    "        'num_classes': 80,\n",
    "        'image_size': (416, 416),\n",
    "        'confidence_threshold': 0.5,\n",
    "        'iou_threshold': 0.5\n",
    "    },\n",
    "    'inference': {\n",
    "        'batch_size': 1,\n",
    "        'use_amp': True,\n",
    "        'warmup_runs': 10,\n",
    "        'benchmark_runs': 100,\n",
    "        'device': 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    },\n",
    "    'visualization': {\n",
    "        'show_confidence': True,\n",
    "        'show_class_names': True,\n",
    "        'color_scheme': 'categorical'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('../results/inference', exist_ok=True)\n",
    "os.makedirs('../results/visualizations', exist_ok=True)\n",
    "\n",
    "# Initialize logger\n",
    "logger = setup_logger('inference_demo')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 2. Model Loading and Preparation\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "class ModelLoader:\n",
    "    \"\"\"Load and prepare model for inference.\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.device = torch.device(config['inference']['device'])\n",
    "        \n",
    "    def load_model(self, model_path=None):\n",
    "        \"\"\"Load model from checkpoint.\"\"\"\n",
    "        if model_path is None:\n",
    "            model_path = self.config['model']['path']\n",
    "        \n",
    "        print(f\"Loading model from: {model_path}\")\n",
    "        \n",
    "        if not os.path.exists(model_path):\n",
    "            print(f\"Model file not found: {model_path}\")\n",
    "            print(\"Creating a new model for demonstration...\")\n",
    "            return self.create_demo_model()\n",
    "        \n",
    "        try:\n",
    "            # Load checkpoint\n",
    "            checkpoint = torch.load(model_path, map_location=self.device)\n",
    "            \n",
    "            # Create model\n",
    "            model = HybridVisionSystem(\n",
    "                config=self.config['model'],\n",
    "                num_classes=self.config['model']['num_classes'],\n",
    "                use_vit=True,\n",
    "                use_rag=False\n",
    "            ).to(self.device)\n",
    "            \n",
    "            # Load weights\n",
    "            if 'model_state_dict' in checkpoint:\n",
    "                model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            else:\n",
    "                model.load_state_dict(checkpoint)\n",
    "            \n",
    "            # Set to evaluation mode\n",
    "            model.eval()\n",
    "            \n",
    "            print(\"✅ Model loaded successfully!\")\n",
    "            \n",
    "            # Print model info\n",
    "            self.print_model_info(model, checkpoint)\n",
    "            \n",
    "            return model\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {e}\")\n",
    "            print(\"Creating a new model for demonstration...\")\n",
    "            return self.create_demo_model()\n",
    "    \n",
    "    def create_demo_model(self):\n",
    "        \"\"\"Create a new model for demonstration.\"\"\"\n",
    "        print(\"Creating new model for demonstration...\")\n",
    "        \n",
    "        model = HybridVisionSystem(\n",
    "            config=self.config['model'],\n",
    "            num_classes=self.config['model']['num_classes'],\n",
    "            use_vit=True,\n",
    "            use_rag=False\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Initialize with random weights\n",
    "        model.eval()\n",
    "        \n",
    "        print(\"✅ Demo model created successfully!\")\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def print_model_info(self, model, checkpoint=None):\n",
    "        \"\"\"Print model information.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"MODEL INFORMATION\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Count parameters\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        \n",
    "        print(f\"Total parameters: {total_params:,}\")\n",
    "        print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "        \n",
    "        # Checkpoint info\n",
    "        if checkpoint is not None:\n",
    "            if 'epoch' in checkpoint:\n",
    "                print(f\"Trained for: {checkpoint['epoch']} epochs\")\n",
    "            if 'loss' in checkpoint:\n",
    "                print(f\"Checkpoint loss: {checkpoint['loss']:.4f}\")\n",
    "        \n",
    "        # Device info\n",
    "        print(f\"Device: {self.device}\")\n",
    "        print(f\"Using mixed precision: {self.config['inference']['use_amp']}\")\n",
    "        \n",
    "        # Model components\n",
    "        print(\"\\nModel Components:\")\n",
    "        for name, module in model.named_children():\n",
    "            if hasattr(module, 'parameters'):\n",
    "                params = sum(p.numel() for p in module.parameters())\n",
    "                print(f\"  {name}: {params:,} parameters\")\n",
    "        \n",
    "        print(\"=\"*50)\n",
    "\n",
    "# %%\n",
    "# Load model\n",
    "model_loader = ModelLoader(config)\n",
    "model = model_loader.load_model()\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 3. Inference Engine Setup\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "class InferenceEngineDemo:\n",
    "    \"\"\"Demonstrate inference engine capabilities.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, config):\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "        self.device = torch.device(config['inference']['device'])\n",
    "        \n",
    "        # Create inference engine\n",
    "        self.engine = InferenceEngine(\n",
    "            model=model,\n",
    "            image_size=config['model']['image_size'],\n",
    "            confidence_threshold=config['model']['confidence_threshold'],\n",
    "            iou_threshold=config['model']['iou_threshold'],\n",
    "            device=self.device\n",
    "        )\n",
    "        \n",
    "        # Create preprocessor\n",
    "        self.preprocessor = ImagePreprocessor(\n",
    "            image_size=config['model']['image_size'],\n",
    "            normalize=True\n",
    "        )\n",
    "        \n",
    "        # Create postprocessor\n",
    "        self.postprocessor = DetectionPostprocessor(\n",
    "            num_classes=config['model']['num_classes'],\n",
    "            confidence_threshold=config['model']['confidence_threshold'],\n",
    "            iou_threshold=config['model']['iou_threshold']\n",
    "        )\n",
    "        \n",
    "        # Create visualizer\n",
    "        self.visualizer = DetectionVisualizer(\n",
    "            class_names=self.get_coco_class_names(),\n",
    "            color_scheme=config['visualization']['color_scheme']\n",
    "        )\n",
    "        \n",
    "    def get_coco_class_names(self):\n",
    "        \"\"\"Get COCO dataset class names.\"\"\"\n",
    "        return [\n",
    "            'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck',\n",
    "            'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench',\n",
    "            'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra',\n",
    "            'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee',\n",
    "            'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',\n",
    "            'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup',\n",
    "            'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange',\n",
    "            'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\n",
    "            'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse',\n",
    "            'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink',\n",
    "            'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier',\n",
    "            'toothbrush'\n",
    "        ]\n",
    "    \n",
    "    def load_sample_images(self, num_images=5):\n",
    "        \"\"\"Load sample images for demonstration.\"\"\"\n",
    "        print(f\"\\nLoading {num_images} sample images...\")\n",
    "        \n",
    "        sample_images = []\n",
    "        \n",
    "        # Try to load from sample directory\n",
    "        sample_dir = Path('../data/samples')\n",
    "        if sample_dir.exists():\n",
    "            image_files = list(sample_dir.glob('*.jpg')) + list(sample_dir.glob('*.png'))\n",
    "            image_files = image_files[:num_images]\n",
    "            \n",
    "            for img_file in image_files:\n",
    "                try:\n",
    "                    image = Image.open(img_file)\n",
    "                    image = np.array(image)\n",
    "                    sample_images.append((str(img_file), image))\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {img_file}: {e}\")\n",
    "        \n",
    "        # If no sample images found, create synthetic ones\n",
    "        if not sample_images:\n",
    "            print(\"No sample images found. Creating synthetic images...\")\n",
    "            \n",
    "            for i in range(num_images):\n",
    "                # Create synthetic image with random shapes\n",
    "                image = np.random.randint(0, 255, (416, 416, 3), dtype=np.uint8)\n",
    "                \n",
    "                # Add some random rectangles to simulate objects\n",
    "                for _ in range(np.random.randint(3, 8)):\n",
    "                    x1 = np.random.randint(50, 366)\n",
    "                    y1 = np.random.randint(50, 366)\n",
    "                    x2 = x1 + np.random.randint(30, 100)\n",
    "                    y2 = y1 + np.random.randint(30, 100)\n",
    "                    color = np.random.randint(0, 255, 3)\n",
    "                    cv2.rectangle(image, (x1, y1), (x2, y2), color.tolist(), -1)\n",
    "                \n",
    "                sample_images.append((f'synthetic_{i}.jpg', image))\n",
    "        \n",
    "        print(f\"Loaded {len(sample_images)} images\")\n",
    "        \n",
    "        return sample_images\n",
    "    \n",
    "    def run_single_inference(self, image_path_or_array):\n",
    "        \"\"\"Run inference on a single image.\"\"\"\n",
    "        print(f\"\\nRunning single image inference...\")\n",
    "        \n",
    "        # Load and preprocess image\n",
    "        if isinstance(image_path_or_array, str):\n",
    "            print(f\"Loading image from: {image_path_or_array}\")\n",
    "            image = Image.open(image_path_or_array)\n",
    "            image_np = np.array(image)\n",
    "        else:\n",
    "            image_np = image_path_or_array\n",
    "        \n",
    "        # Display original image\n",
    "        print(f\"Original image shape: {image_np.shape}\")\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "        axes[0].imshow(image_np)\n",
    "        axes[0].set_title('Original Image')\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        # Run inference\n",
    "        start_time = time.time()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Preprocess\n",
    "            input_tensor = self.preprocessor(image_np)\n",
    "            \n",
    "            # Inference\n",
    "            if self.config['inference']['use_amp']:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = self.engine.infer(input_tensor)\n",
    "            else:\n",
    "                outputs = self.engine.infer(input_tensor)\n",
    "        \n",
    "        inference_time = (time.time() - start_time) * 1000  # Convert to ms\n",
    "        \n",
    "        # Postprocess\n",
    "        detections = self.postprocessor(outputs)\n",
    "        \n",
    "        print(f\"Inference time: {inference_time:.2f} ms\")\n",
    "        print(f\"Detected objects: {len(detections['boxes'])}\")\n",
    "        \n",
    "        # Visualize results\n",
    "        visualized = self.visualizer.visualize_detections(\n",
    "            image_np, \n",
    "            detections,\n",
    "            show_confidence=self.config['visualization']['show_confidence'],\n",
    "            show_class_names=self.config['visualization']['show_class_names']\n",
    "        )\n",
    "        \n",
    "        axes[1].imshow(visualized)\n",
    "        axes[1].set_title(f'Detections ({len(detections[\"boxes\"])} objects)')\n",
    "        axes[1].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print detection details\n",
    "        if len(detections['boxes']) > 0:\n",
    "            print(\"\\nDetection Details:\")\n",
    "            print(\"-\" * 80)\n",
    "            print(f\"{'Class':<20} {'Confidence':<12} {'BBox (x1,y1,x2,y2)':<30}\")\n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "            for i in range(min(5, len(detections['boxes']))):  # Show top 5\n",
    "                class_name = detections['class_names'][i]\n",
    "                confidence = detections['scores'][i]\n",
    "                bbox = detections['boxes'][i]\n",
    "                \n",
    "                print(f\"{class_name:<20} {confidence:<12.4f} {str(bbox):<30}\")\n",
    "        \n",
    "        return {\n",
    "            'image': image_np,\n",
    "            'detections': detections,\n",
    "            'inference_time': inference_time\n",
    "        }\n",
    "    \n",
    "    def run_batch_inference(self, images, batch_size=4):\n",
    "        \"\"\"Run inference on a batch of images.\"\"\"\n",
    "        print(f\"\\nRunning batch inference (batch_size={batch_size})...\")\n",
    "        \n",
    "        if len(images) < batch_size:\n",
    "            print(f\"Warning: Only {len(images)} images available, using batch_size={len(images)}\")\n",
    "            batch_size = len(images)\n",
    "        \n",
    "        # Preprocess images\n",
    "        batch_tensors = []\n",
    "        original_images = []\n",
    "        \n",
    "        for img_name, img_array in images[:batch_size]:\n",
    "            input_tensor = self.preprocessor(img_array)\n",
    "            batch_tensors.append(input_tensor)\n",
    "            original_images.append((img_name, img_array))\n",
    "        \n",
    "        # Stack batch\n",
    "        batch_tensor = torch.cat(batch_tensors, dim=0)\n",
    "        \n",
    "        print(f\"Batch tensor shape: {batch_tensor.shape}\")\n",
    "        \n",
    "        # Run inference\n",
    "        start_time = time.time()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            if self.config['inference']['use_amp']:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = self.engine.infer_batch(batch_tensor)\n",
    "            else:\n",
    "                outputs = self.engine.infer_batch(batch_tensor)\n",
    "        \n",
    "        batch_time = (time.time() - start_time) * 1000  # Convert to ms\n",
    "        \n",
    "        print(f\"Batch inference time: {batch_time:.2f} ms\")\n",
    "        print(f\"Time per image: {batch_time/batch_size:.2f} ms\")\n",
    "        \n",
    "        # Process results for each image\n",
    "        all_results = []\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            img_name, img_array = original_images[i]\n",
    "            \n",
    "            # Extract outputs for this image\n",
    "            if isinstance(outputs, dict):\n",
    "                # Handle dictionary outputs\n",
    "                image_outputs = {k: v[i] if v is not None else None \n",
    "                               for k, v in outputs.items()}\n",
    "            else:\n",
    "                # Handle tensor outputs\n",
    "                image_outputs = outputs[i]\n",
    "            \n",
    "            # Postprocess\n",
    "            detections = self.postprocessor(image_outputs)\n",
    "            \n",
    "            all_results.append({\n",
    "                'image_name': img_name,\n",
    "                'image': img_array,\n",
    "                'detections': detections\n",
    "            })\n",
    "            \n",
    "            print(f\"\\nImage {i+1}: {img_name}\")\n",
    "            print(f\"  Detected objects: {len(detections['boxes'])}\")\n",
    "        \n",
    "        # Visualize batch results\n",
    "        self.visualize_batch_results(all_results, batch_size)\n",
    "        \n",
    "        return all_results, batch_time\n",
    "    \n",
    "    def visualize_batch_results(self, results, batch_size):\n",
    "        \"\"\"Visualize batch inference results.\"\"\"\n",
    "        fig, axes = plt.subplots(2, min(4, batch_size), figsize=(16, 8))\n",
    "        \n",
    "        if batch_size == 1:\n",
    "            axes = axes.reshape(2, 1)\n",
    "        \n",
    "        for i in range(min(4, batch_size)):\n",
    "            result = results[i]\n",
    "            img_array = result['image']\n",
    "            detections = result['detections']\n",
    "            \n",
    "            # Original image\n",
    "            axes[0, i].imshow(img_array)\n",
    "            axes[0, i].set_title(f'Original {i+1}')\n",
    "            axes[0, i].axis('off')\n",
    "            \n",
    "            # Detected image\n",
    "            visualized = self.visualizer.visualize_detections(\n",
    "                img_array,\n",
    "                detections,\n",
    "                show_confidence=True,\n",
    "                show_class_names=True\n",
    "            )\n",
    "            \n",
    "            axes[1, i].imshow(visualized)\n",
    "            axes[1, i].set_title(f'Detected: {len(detections[\"boxes\"])} objects')\n",
    "            axes[1, i].axis('off')\n",
    "        \n",
    "        plt.suptitle(f'Batch Inference Results (Batch Size: {batch_size})', fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# %%\n",
    "# Create inference engine\n",
    "inference_demo = InferenceEngineDemo(model, config)\n",
    "\n",
    "# Load sample images\n",
    "sample_images = inference_demo.load_sample_images(num_images=5)\n",
    "\n",
    "# Run single inference on first image\n",
    "if sample_images:\n",
    "    first_image_path, first_image_array = sample_images[0]\n",
    "    result = inference_demo.run_single_inference(first_image_array)\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 4. Performance Benchmarking\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "class PerformanceBenchmark:\n",
    "    \"\"\"Benchmark inference performance.\"\"\"\n",
    "    \n",
    "    def __init__(self, inference_engine, config):\n",
    "        self.engine = inference_engine\n",
    "        self.config = config\n",
    "        self.device = torch.device(config['inference']['device'])\n",
    "        \n",
    "    def benchmark_latency(self, num_runs=100, warmup_runs=10, batch_size=1):\n",
    "        \"\"\"Benchmark inference latency.\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"LATENCY BENCHMARK\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Runs: {num_runs} (warmup: {warmup_runs})\")\n",
    "        print(f\"Batch size: {batch_size}\")\n",
    "        print(f\"Device: {self.device}\")\n",
    "        print(f\"Mixed precision: {self.config['inference']['use_amp']}\")\n",
    "        \n",
    "        # Create test input\n",
    "        H, W = self.config['model']['image_size']\n",
    "        test_input = torch.randn(batch_size, 3, H, W).to(self.device)\n",
    "        \n",
    "        latencies = []\n",
    "        \n",
    "        # Warmup runs\n",
    "        print(f\"\\nWarmup runs ({warmup_runs})...\")\n",
    "        for i in range(warmup_runs):\n",
    "            with torch.no_grad():\n",
    "                if self.config['inference']['use_amp']:\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        _ = self.engine.model(test_input)\n",
    "                else:\n",
    "                    _ = self.engine.model(test_input)\n",
    "            \n",
    "            if (i + 1) % 5 == 0:\n",
    "                print(f\"  Warmup run {i + 1}/{warmup_runs}\")\n",
    "        \n",
    "        # Benchmark runs\n",
    "        print(f\"\\nBenchmark runs ({num_runs})...\")\n",
    "        for i in range(num_runs):\n",
    "            torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "            start_time = time.perf_counter()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                if self.config['inference']['use_amp']:\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        _ = self.engine.model(test_input)\n",
    "                else:\n",
    "                    _ = self.engine.model(test_input)\n",
    "            \n",
    "            torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "            end_time = time.perf_counter()\n",
    "            \n",
    "            latency = (end_time - start_time) * 1000  # Convert to ms\n",
    "            latencies.append(latency)\n",
    "            \n",
    "            if (i + 1) % 20 == 0:\n",
    "                print(f\"  Run {i + 1}/{num_runs}: {latency:.2f} ms\")\n",
    "        \n",
    "        # Calculate statistics\n",
    "        latencies = np.array(latencies)\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"RESULTS:\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Mean latency: {latencies.mean():.2f} ms\")\n",
    "        print(f\"Std latency: {latencies.std():.2f} ms\")\n",
    "        print(f\"Min latency: {latencies.min():.2f} ms\")\n",
    "        print(f\"Max latency: {latencies.max():.2f} ms\")\n",
    "        print(f\"Median latency: {np.median(latencies):.2f} ms\")\n",
    "        print(f\"Throughput: {1000/latencies.mean():.1f} FPS\")\n",
    "        \n",
    "        # Percentiles\n",
    "        percentiles = [50, 90, 95, 99]\n",
    "        print(f\"\\nLatency Percentiles:\")\n",
    "        for p in percentiles:\n",
    "            value = np.percentile(latencies, p)\n",
    "            print(f\"  P{p}: {value:.2f} ms\")\n",
    "        \n",
    "        # Visualize results\n",
    "        self.visualize_latency_benchmark(latencies, batch_size)\n",
    "        \n",
    "        return latencies\n",
    "    \n",
    "    def visualize_latency_benchmark(self, latencies, batch_size):\n",
    "        \"\"\"Visualize latency benchmark results.\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "        \n",
    "        # Histogram\n",
    "        axes[0, 0].hist(latencies, bins=30, alpha=0.7, edgecolor='black')\n",
    "        axes[0, 0].axvline(np.mean(latencies), color='red', linestyle='--',\n",
    "                          label=f'Mean: {np.mean(latencies):.2f} ms')\n",
    "        axes[0, 0].axvline(np.median(latencies), color='green', linestyle='--',\n",
    "                          label=f'Median: {np.median(latencies):.2f} ms')\n",
    "        axes[0, 0].set_xlabel('Latency (ms)')\n",
    "        axes[0, 0].set_ylabel('Frequency')\n",
    "        axes[0, 0].set_title('Latency Distribution')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Time series\n",
    "        axes[0, 1].plot(latencies, marker='o', linestyle='-', alpha=0.6)\n",
    "        axes[0, 1].axhline(np.mean(latencies), color='red', linestyle='--',\n",
    "                          label=f'Mean: {np.mean(latencies):.2f} ms')\n",
    "        axes[0, 1].set_xlabel('Run')\n",
    "        axes[0, 1].set_ylabel('Latency (ms)')\n",
    "        axes[0, 1].set_title('Latency Over Time')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # CDF\n",
    "        sorted_latencies = np.sort(latencies)\n",
    "        cdf = np.arange(1, len(sorted_latencies) + 1) / len(sorted_latencies)\n",
    "        axes[1, 0].plot(sorted_latencies, cdf, linewidth=2)\n",
    "        axes[1, 0].set_xlabel('Latency (ms)')\n",
    "        axes[1, 0].set_ylabel('CDF')\n",
    "        axes[1, 0].set_title('Cumulative Distribution Function')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add percentile lines\n",
    "        percentiles = [50, 90, 95, 99]\n",
    "        colors = ['red', 'orange', 'green', 'blue']\n",
    "        for p, color in zip(percentiles, colors):\n",
    "            value = np.percentile(latencies, p)\n",
    "            axes[1, 0].axvline(value, color=color, linestyle='--', alpha=0.7,\n",
    "                              label=f'P{p}: {value:.1f} ms')\n",
    "        axes[1, 0].legend()\n",
    "        \n",
    "        # Box plot\n",
    "        axes[1, 1].boxplot(latencies, vert=True, patch_artist=True)\n",
    "        axes[1, 1].set_ylabel('Latency (ms)')\n",
    "        axes[1, 1].set_title('Box Plot')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add statistics text\n",
    "        stats_text = f\"\"\"\n",
    "        Batch Size: {batch_size}\n",
    "        Mean: {np.mean(latencies):.2f} ms\n",
    "        Std: {np.std(latencies):.2f} ms\n",
    "        Min: {np.min(latencies):.2f} ms\n",
    "        Max: {np.max(latencies):.2f} ms\n",
    "        FPS: {1000/np.mean(latencies):.1f}\n",
    "        \"\"\"\n",
    "        axes[1, 1].text(0.95, 0.05, stats_text,\n",
    "                       transform=axes[1, 1].transAxes,\n",
    "                       verticalalignment='bottom',\n",
    "                       horizontalalignment='right',\n",
    "                       bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "        \n",
    "        plt.suptitle(f'Inference Latency Benchmark (Batch Size: {batch_size})', fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Create interactive visualization\n",
    "        self.create_interactive_benchmark_viz(latencies, batch_size)\n",
    "    \n",
    "    def create_interactive_benchmark_viz(self, latencies, batch_size):\n",
    "        \"\"\"Create interactive benchmark visualization.\"\"\"\n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=2,\n",
    "            subplot_titles=('Latency Distribution', 'Latency Over Time',\n",
    "                           'Cumulative Distribution', 'Box Plot'),\n",
    "            vertical_spacing=0.1\n",
    "        )\n",
    "        \n",
    "        # Histogram\n",
    "        fig.add_trace(\n",
    "            go.Histogram(x=latencies, nbinsx=30, name='Distribution',\n",
    "                        marker_color='blue'),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # Add mean and median lines\n",
    "        fig.add_vline(x=np.mean(latencies), line_dash=\"dash\", line_color=\"red\",\n",
    "                     annotation_text=f\"Mean: {np.mean(latencies):.2f} ms\", row=1, col=1)\n",
    "        fig.add_vline(x=np.median(latencies), line_dash=\"dash\", line_color=\"green\",\n",
    "                     annotation_text=f\"Median: {np.median(latencies):.2f} ms\", row=1, col=1)\n",
    "        \n",
    "        # Time series\n",
    "        fig.add_trace(\n",
    "            go.Scatter(y=latencies, mode='lines+markers', name='Latency',\n",
    "                      line=dict(color='orange')),\n",
    "            row=1, col=2\n",
    "        )\n",
    "        \n",
    "        fig.add_hline(y=np.mean(latencies), line_dash=\"dash\", line_color=\"red\",\n",
    "                     annotation_text=f\"Mean\", row=1, col=2)\n",
    "        \n",
    "        # CDF\n",
    "        sorted_latencies = np.sort(latencies)\n",
    "        cdf = np.arange(1, len(sorted_latencies) + 1) / len(sorted_latencies)\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=sorted_latencies, y=cdf, mode='lines', name='CDF',\n",
    "                      line=dict(width=3, color='purple')),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        \n",
    "        # Add percentile lines\n",
    "        percentiles = [50, 90, 95, 99]\n",
    "        percentile_values = [np.percentile(latencies, p) for p in percentiles]\n",
    "        percentile_labels = [f'P{p}: {v:.1f} ms' for p, v in zip(percentiles, percentile_values)]\n",
    "        \n",
    "        for value, label in zip(percentile_values, percentile_labels):\n",
    "            fig.add_vline(x=value, line_dash=\"dash\", line_color=\"red\", row=2, col=1,\n",
    "                         annotation_text=label)\n",
    "        \n",
    "        # Box plot\n",
    "        fig.add_trace(\n",
    "            go.Box(y=latencies, name='Latency', boxmean='sd',\n",
    "                  marker_color='lightblue'),\n",
    "            row=2, col=2\n",
    "        )\n",
    "        \n",
    "        fig.update_layout(\n",
    "            height=800,\n",
    "            width=1200,\n",
    "            title_text=f\"Inference Latency Benchmark (Batch Size: {batch_size})\",\n",
    "            showlegend=True\n",
    "        )\n",
    "        \n",
    "        fig.show()\n",
    "    \n",
    "    def benchmark_memory(self, batch_sizes=[1, 2, 4, 8, 16]):\n",
    "        \"\"\"Benchmark memory usage for different batch sizes.\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"MEMORY BENCHMARK\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Batch sizes: {batch_sizes}\")\n",
    "        print(f\"Device: {self.device}\")\n",
    "        \n",
    "        memory_results = []\n",
    "        \n",
    "        for batch_size in batch_sizes:\n",
    "            print(f\"\\nTesting batch size: {batch_size}\")\n",
    "            \n",
    "            # Clear cache\n",
    "            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "            \n",
    "            # Measure memory before\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.reset_peak_memory_stats()\n",
    "                start_memory = torch.cuda.memory_allocated()\n",
    "            else:\n",
    "                start_memory = psutil.Process().memory_info().rss / 1024**2  # MB\n",
    "            \n",
    "            # Create test input\n",
    "            H, W = self.config['model']['image_size']\n",
    "            test_input = torch.randn(batch_size, 3, H, W).to(self.device)\n",
    "            \n",
    "            # Run inference to allocate memory\n",
    "            with torch.no_grad():\n",
    "                if self.config['inference']['use_amp']:\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        _ = self.engine.model(test_input)\n",
    "                else:\n",
    "                    _ = self.engine.model(test_input)\n",
    "            \n",
    "            # Measure memory after\n",
    "            if torch.cuda.is_available():\n",
    "                peak_memory = torch.cuda.max_memory_allocated()\n",
    "                memory_used = (peak_memory - start_memory) / 1024**3  # Convert to GB\n",
    "            else:\n",
    "                end_memory = psutil.Process().memory_info().rss / 1024**2\n",
    "                memory_used = (end_memory - start_memory) / 1024  # Convert to GB\n",
    "            \n",
    "            memory_results.append((batch_size, memory_used))\n",
    "            \n",
    "            print(f\"  Memory used: {memory_used:.3f} GB\")\n",
    "            \n",
    "            # Clear cache\n",
    "            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "        \n",
    "        # Analyze results\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"MEMORY USAGE ANALYSIS:\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        batch_sizes_arr = np.array([r[0] for r in memory_results])\n",
    "        memory_used_arr = np.array([r[1] for r in memory_results])\n",
    "        \n",
    "        for bs, mem in zip(batch_sizes_arr, memory_used_arr):\n",
    "            print(f\"Batch size {bs}: {mem:.3f} GB\")\n",
    "        \n",
    "        # Calculate memory per sample\n",
    "        memory_per_sample = memory_used_arr / batch_sizes_arr\n",
    "        print(f\"\\nMemory per sample: {np.mean(memory_per_sample):.3f} GB\")\n",
    "        \n",
    "        # Fit linear model\n",
    "        coeffs = np.polyfit(batch_sizes_arr, memory_used_arr, 1)\n",
    "        print(f\"Memory model: {coeffs[0]:.3f} * batch_size + {coeffs[1]:.3f}\")\n",
    "        \n",
    "        # Visualize\n",
    "        self.visualize_memory_benchmark(batch_sizes_arr, memory_used_arr, coeffs)\n",
    "        \n",
    "        return memory_results\n",
    "    \n",
    "    def visualize_memory_benchmark(self, batch_sizes, memory_used, coeffs):\n",
    "        \"\"\"Visualize memory benchmark results.\"\"\"\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        \n",
    "        # Memory vs batch size\n",
    "        axes[0].plot(batch_sizes, memory_used, 'bo-', linewidth=2, markersize=8)\n",
    "        \n",
    "        # Linear fit\n",
    "        x_fit = np.linspace(min(batch_sizes), max(batch_sizes), 100)\n",
    "        y_fit = coeffs[0] * x_fit + coeffs[1]\n",
    "        axes[0].plot(x_fit, y_fit, 'r--', label=f'Fit: {coeffs[0]:.3f}x + {coeffs[1]:.3f}')\n",
    "        \n",
    "        axes[0].set_xlabel('Batch Size')\n",
    "        axes[0].set_ylabel('Memory Used (GB)')\n",
    "        axes[0].set_title('Memory Usage vs Batch Size')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add annotations\n",
    "        for bs, mem in zip(batch_sizes, memory_used):\n",
    "            axes[0].annotate(f'{mem:.3f} GB', \n",
    "                           xy=(bs, mem), \n",
    "                           xytext=(0, 10),\n",
    "                           textcoords='offset points',\n",
    "                           ha='center')\n",
    "        \n",
    "        # Memory per sample\n",
    "        memory_per_sample = memory_used / batch_sizes\n",
    "        \n",
    "        axes[1].bar(range(len(batch_sizes)), memory_per_sample)\n",
    "        axes[1].set_xlabel('Batch Size Index')\n",
    "        axes[1].set_ylabel('Memory per Sample (GB)')\n",
    "        axes[1].set_title('Memory Efficiency')\n",
    "        axes[1].set_xticks(range(len(batch_sizes)))\n",
    "        axes[1].set_xticklabels([f'BS={bs}' for bs in batch_sizes])\n",
    "        axes[1].grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, mem_per in enumerate(memory_per_sample):\n",
    "            axes[1].text(i, mem_per, f'{mem_per:.3f}', \n",
    "                        ha='center', va='bottom')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def benchmark_batch_sizes(self, max_batch_size=32):\n",
    "        \"\"\"Find optimal batch size for throughput.\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"BATCH SIZE OPTIMIZATION\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        batch_sizes = [1, 2, 4, 8, 16, 32]\n",
    "        batch_sizes = [bs for bs in batch_sizes if bs <= max_batch_size]\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for batch_size in batch_sizes:\n",
    "            print(f\"\\nBenchmarking batch size: {batch_size}\")\n",
    "            \n",
    "            # Benchmark latency for this batch size\n",
    "            latencies = self.benchmark_latency(\n",
    "                num_runs=50,\n",
    "                warmup_runs=5,\n",
    "                batch_size=batch_size\n",
    "            )\n",
    "            \n",
    "            mean_latency = np.mean(latencies)\n",
    "            throughput = 1000 / mean_latency * batch_size  # Images per second\n",
    "            \n",
    "            results.append({\n",
    "                'batch_size': batch_size,\n",
    "                'mean_latency': mean_latency,\n",
    "                'throughput': throughput,\n",
    "                'efficiency': throughput / batch_size  # Throughput per GPU unit\n",
    "            })\n",
    "            \n",
    "            print(f\"  Throughput: {throughput:.1f} images/sec\")\n",
    "            print(f\"  Efficiency: {throughput/batch_size:.1f} images/sec per batch unit\")\n",
    "        \n",
    "        # Find optimal batch size\n",
    "        df_results = pd.DataFrame(results)\n",
    "        optimal_idx = df_results['throughput'].idxmax()\n",
    "        optimal = df_results.iloc[optimal_idx]\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"OPTIMAL BATCH SIZE ANALYSIS:\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Optimal batch size: {optimal['batch_size']}\")\n",
    "        print(f\"Max throughput: {optimal['throughput']:.1f} images/sec\")\n",
    "        print(f\"Latency at optimal: {optimal['mean_latency']:.2f} ms\")\n",
    "        \n",
    "        # Visualize\n",
    "        self.visualize_batch_optimization(df_results)\n",
    "        \n",
    "        return df_results, optimal\n",
    "    \n",
    "    def visualize_batch_optimization(self, df_results):\n",
    "        \"\"\"Visualize batch size optimization results.\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "        \n",
    "        # Throughput vs batch size\n",
    "        axes[0, 0].plot(df_results['batch_size'], df_results['throughput'], 'bo-', \n",
    "                       linewidth=2, markersize=8)\n",
    "        axes[0, 0].set_xlabel('Batch Size')\n",
    "        axes[0, 0].set_ylabel('Throughput (images/sec)')\n",
    "        axes[0, 0].set_title('Throughput vs Batch Size')\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Mark optimal point\n",
    "        optimal_idx = df_results['throughput'].idxmax()\n",
    "        optimal_bs = df_results.loc[optimal_idx, 'batch_size']\n",
    "        optimal_throughput = df_results.loc[optimal_idx, 'throughput']\n",
    "        axes[0, 0].plot(optimal_bs, optimal_throughput, 'r*', markersize=15, \n",
    "                       label=f'Optimal: BS={optimal_bs}')\n",
    "        axes[0, 0].legend()\n",
    "        \n",
    "        # Latency vs batch size\n",
    "        axes[0, 1].plot(df_results['batch_size'], df_results['mean_latency'], 'ro-',\n",
    "                       linewidth=2, markersize=8)\n",
    "        axes[0, 1].set_xlabel('Batch Size')\n",
    "        axes[0, 1].set_ylabel('Latency (ms)')\n",
    "        axes[0, 1].set_title('Latency vs Batch Size')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Efficiency vs batch size\n",
    "        axes[1, 0].plot(df_results['batch_size'], df_results['efficiency'], 'go-',\n",
    "                       linewidth=2, markersize=8)\n",
    "        axes[1, 0].set_xlabel('Batch Size')\n",
    "        axes[1, 0].set_ylabel('Efficiency (images/sec per batch unit)')\n",
    "        axes[1, 0].set_title('Efficiency vs Batch Size')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Throughput-Latency tradeoff\n",
    "        axes[1, 1].scatter(df_results['mean_latency'], df_results['throughput'],\n",
    "                          c=df_results['batch_size'], s=100, cmap='viridis')\n",
    "        axes[1, 1].set_xlabel('Latency (ms)')\n",
    "        axes[1, 1].set_ylabel('Throughput (images/sec)')\n",
    "        axes[1, 1].set_title('Throughput-Latency Tradeoff')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add batch size labels\n",
    "        for i, row in df_results.iterrows():\n",
    "            axes[1, 1].annotate(f\"BS={row['batch_size']}\", \n",
    "                              xy=(row['mean_latency'], row['throughput']),\n",
    "                              xytext=(5, 5), textcoords='offset points')\n",
    "        \n",
    "        # Colorbar for batch sizes\n",
    "        plt.colorbar(axes[1, 1].collections[0], ax=axes[1, 1], label='Batch Size')\n",
    "        \n",
    "        plt.suptitle('Batch Size Optimization Analysis', fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# %%\n",
    "# Run performance benchmarks\n",
    "benchmark = PerformanceBenchmark(inference_demo.engine, config)\n",
    "\n",
    "# Benchmark latency\n",
    "latencies = benchmark.benchmark_latency(\n",
    "    num_runs=config['inference']['benchmark_runs'],\n",
    "    warmup_runs=config['inference']['warmup_runs'],\n",
    "    batch_size=config['inference']['batch_size']\n",
    ")\n",
    "\n",
    "# Benchmark memory\n",
    "memory_results = benchmark.benchmark_memory(batch_sizes=[1, 2, 4, 8, 16])\n",
    "\n",
    "# Find optimal batch size\n",
    "batch_results, optimal = benchmark.benchmark_batch_sizes(max_batch_size=16)\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 5. Real-time Video Inference Demo\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "class VideoInferenceDemo:\n",
    "    \"\"\"Demonstrate real-time video inference.\"\"\"\n",
    "    \n",
    "    def __init__(self, inference_engine, config):\n",
    "        self.engine = inference_engine\n",
    "        self.config = config\n",
    "        self.device = torch.device(config['inference']['device'])\n",
    "        \n",
    "        # Create visualizer\n",
    "        self.visualizer = DetectionVisualizer(\n",
    "            class_names=inference_engine.visualizer.class_names,\n",
    "            color_scheme=config['visualization']['color_scheme']\n",
    "        )\n",
    "        \n",
    "        # Statistics\n",
    "        self.stats = {\n",
    "            'frame_count': 0,\n",
    "            'total_time': 0,\n",
    "            'fps_history': [],\n",
    "            'detection_history': []\n",
    "        }\n",
    "    \n",
    "    def process_video_file(self, video_path, max_frames=100, output_path=None):\n",
    "        \"\"\"Process video file and display results.\"\"\"\n",
    "        print(f\"\\nProcessing video: {video_path}\")\n",
    "        print(f\"Max frames: {max_frames}\")\n",
    "        \n",
    "        # Open video\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            print(f\"Error opening video: {video_path}\")\n",
    "            print(\"Using synthetic video instead...\")\n",
    "            return self.create_synthetic_video(max_frames)\n",
    "        \n",
    "        # Get video properties\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        \n",
    "        print(f\"Video properties: {width}x{height}, {fps:.1f} FPS, {total_frames} frames\")\n",
    "        \n",
    "        # Prepare output writer\n",
    "        if output_path:\n",
    "            fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "            out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "        else:\n",
    "            out = None\n",
    "        \n",
    "        # Process frames\n",
    "        frames_processed = 0\n",
    "        processed_frames = []\n",
    "        \n",
    "        while frames_processed < max_frames:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            # Convert BGR to RGB\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Run inference\n",
    "            start_time = time.time()\n",
    "            \n",
    "            input_tensor = self.engine.preprocessor(frame_rgb)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                if self.config['inference']['use_amp']:\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        outputs = self.engine.engine.infer(input_tensor)\n",
    "                else:\n",
    "                    outputs = self.engine.engine.infer(input_tensor)\n",
    "            \n",
    "            inference_time = (time.time() - start_time) * 1000\n",
    "            \n",
    "            # Postprocess\n",
    "            detections = self.engine.postprocessor(outputs)\n",
    "            \n",
    "            # Visualize\n",
    "            visualized = self.visualizer.visualize_detections(\n",
    "                frame_rgb,\n",
    "                detections,\n",
    "                show_confidence=True,\n",
    "                show_class_names=True\n",
    "            )\n",
    "            \n",
    "            # Convert back to BGR for OpenCV\n",
    "            visualized_bgr = cv2.cvtColor(visualized, cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "            # Add FPS counter\n",
    "            fps_text = f\"FPS: {1000/inference_time:.1f}\" if inference_time > 0 else \"FPS: N/A\"\n",
    "            cv2.putText(visualized_bgr, fps_text, (10, 30),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            \n",
    "            # Add detection count\n",
    "            det_text = f\"Detections: {len(detections['boxes'])}\"\n",
    "            cv2.putText(visualized_bgr, det_text, (10, 70),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            \n",
    "            # Store processed frame\n",
    "            processed_frames.append(visualized_bgr)\n",
    "            \n",
    "            # Write to output\n",
    "            if out:\n",
    "                out.write(visualized_bgr)\n",
    "            \n",
    "            # Update statistics\n",
    "            self.stats['frame_count'] += 1\n",
    "            self.stats['total_time'] += inference_time\n",
    "            self.stats['fps_history'].append(1000/inference_time if inference_time > 0 else 0)\n",
    "            self.stats['detection_history'].append(len(detections['boxes']))\n",
    "            \n",
    "            frames_processed += 1\n",
    "            \n",
    "            if frames_processed % 10 == 0:\n",
    "                print(f\"  Processed {frames_processed}/{min(max_frames, total_frames)} frames\")\n",
    "        \n",
    "        # Release resources\n",
    "        cap.release()\n",
    "        if out:\n",
    "            out.release()\n",
    "        \n",
    "        print(f\"\\nVideo processing completed!\")\n",
    "        print(f\"Frames processed: {frames_processed}\")\n",
    "        \n",
    "        # Display sample frames\n",
    "        self.display_sample_frames(processed_frames)\n",
    "        \n",
    "        # Analyze performance\n",
    "        self.analyze_video_performance()\n",
    "        \n",
    "        return processed_frames\n",
    "    \n",
    "    def create_synthetic_video(self, num_frames=50):\n",
    "        \"\"\"Create synthetic video for demonstration.\"\"\"\n",
    "        print(\"Creating synthetic video...\")\n",
    "        \n",
    "        processed_frames = []\n",
    "        \n",
    "        for i in range(num_frames):\n",
    "            # Create synthetic frame\n",
    "            frame = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)\n",
    "            \n",
    "            # Add moving rectangle\n",
    "            x = int(300 + 200 * np.sin(i * 0.1))\n",
    "            y = int(200 + 100 * np.cos(i * 0.1))\n",
    "            cv2.rectangle(frame, (x, y), (x+100, y+100), (0, 255, 0), -1)\n",
    "            \n",
    "            # Add static rectangle\n",
    "            cv2.rectangle(frame, (100, 100), (200, 200), (255, 0, 0), -1)\n",
    "            \n",
    "            # Run inference (simulated for demo)\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Create simulated detections\n",
    "            detections = {\n",
    "                'boxes': [\n",
    "                    [x/640, y/480, (x+100)/640, (y+100)/480],  # Moving object\n",
    "                    [100/640, 100/480, 200/640, 200/480]       # Static object\n",
    "                ],\n",
    "                'scores': [0.9, 0.8],\n",
    "                'class_ids': [0, 1],\n",
    "                'class_names': ['person', 'car']\n",
    "            }\n",
    "            \n",
    "            # Visualize\n",
    "            visualized = self.visualizer.visualize_detections(\n",
    "                frame_rgb,\n",
    "                detections,\n",
    "                show_confidence=True,\n",
    "                show_class_names=True\n",
    "            )\n",
    "            \n",
    "            visualized_bgr = cv2.cvtColor(visualized, cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "            # Add frame info\n",
    "            cv2.putText(visualized_bgr, f\"Frame {i+1}\", (10, 30),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            cv2.putText(visualized_bgr, \"SYNTHETIC DEMO\", (10, 70),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            \n",
    "            processed_frames.append(visualized_bgr)\n",
    "        \n",
    "        print(f\"Created {num_frames} synthetic frames\")\n",
    "        self.display_sample_frames(processed_frames)\n",
    "        \n",
    "        return processed_frames\n",
    "    \n",
    "    def display_sample_frames(self, frames, sample_count=4):\n",
    "        \"\"\"Display sample frames from video.\"\"\"\n",
    "        if not frames:\n",
    "            return\n",
    "        \n",
    "        print(f\"\\nDisplaying {sample_count} sample frames...\")\n",
    "        \n",
    "        fig, axes = plt.subplots(1, min(sample_count, len(frames)), figsize=(16, 4))\n",
    "        \n",
    "        if sample_count == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        indices = np.linspace(0, len(frames)-1, sample_count, dtype=int)\n",
    "        \n",
    "        for idx, ax_idx in enumerate(indices):\n",
    "            frame = frames[ax_idx]\n",
    "            \n",
    "            # Convert BGR to RGB for display\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            axes[idx].imshow(frame_rgb)\n",
    "            axes[idx].set_title(f'Frame {ax_idx + 1}')\n",
    "            axes[idx].axis('off')\n",
    "        \n",
    "        plt.suptitle('Video Inference Results', fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def analyze_video_performance(self):\n",
    "        \"\"\"Analyze video inference performance.\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"VIDEO INFERENCE PERFORMANCE ANALYSIS\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        if not self.stats['fps_history']:\n",
    "            print(\"No performance data available.\")\n",
    "            return\n",
    "        \n",
    "        fps_history = np.array(self.stats['fps_history'])\n",
    "        detection_history = np.array(self.stats['detection_history'])\n",
    "        \n",
    "        print(f\"Frames processed: {self.stats['frame_count']}\")\n",
    "        print(f\"Total time: {self.stats['total_time']/1000:.2f} seconds\")\n",
    "        print(f\"Average FPS: {np.mean(fps_history):.1f}\")\n",
    "        print(f\"Std FPS: {np.std(fps_history):.1f}\")\n",
    "        print(f\"Min FPS: {np.min(fps_history):.1f}\")\n",
    "        print(f\"Max FPS: {np.max(fps_history):.1f}\")\n",
    "        print(f\"Average detections per frame: {np.mean(detection_history):.1f}\")\n",
    "        \n",
    "        # Visualize performance\n",
    "        self.visualize_video_performance(fps_history, detection_history)\n",
    "    \n",
    "    def visualize_video_performance(self, fps_history, detection_history):\n",
    "        \"\"\"Visualize video inference performance.\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "        \n",
    "        # FPS over time\n",
    "        axes[0, 0].plot(fps_history, linewidth=2)\n",
    "        axes[0, 0].axhline(np.mean(fps_history), color='red', linestyle='--',\n",
    "                          label=f'Mean: {np.mean(fps_history):.1f} FPS')\n",
    "        axes[0, 0].set_xlabel('Frame')\n",
    "        axes[0, 0].set_ylabel('FPS')\n",
    "        axes[0, 0].set_title('FPS Over Time')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # FPS distribution\n",
    "        axes[0, 1].hist(fps_history, bins=30, alpha=0.7, edgecolor='black')\n",
    "        axes[0, 1].axvline(np.mean(fps_history), color='red', linestyle='--',\n",
    "                          label=f'Mean: {np.mean(fps_history):.1f}')\n",
    "        axes[0, 1].axvline(np.median(fps_history), color='green', linestyle='--',\n",
    "                          label=f'Median: {np.median(fps_history):.1f}')\n",
    "        axes[0, 1].set_xlabel('FPS')\n",
    "        axes[0, 1].set_ylabel('Frequency')\n",
    "        axes[0, 1].set_title('FPS Distribution')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Detections over time\n",
    "        axes[1, 0].plot(detection_history, linewidth=2, color='orange')\n",
    "        axes[1, 0].axhline(np.mean(detection_history), color='red', linestyle='--',\n",
    "                          label=f'Mean: {np.mean(detection_history):.1f}')\n",
    "        axes[1, 0].set_xlabel('Frame')\n",
    "        axes[1, 0].set_ylabel('Number of Detections')\n",
    "        axes[1, 0].set_title('Detections Over Time')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # FPS vs Detections scatter\n",
    "        axes[1, 1].scatter(detection_history, fps_history, alpha=0.6)\n",
    "        axes[1, 1].set_xlabel('Number of Detections')\n",
    "        axes[1, 1].set_ylabel('FPS')\n",
    "        axes[1, 1].set_title('FPS vs Detections')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add correlation coefficient\n",
    "        correlation = np.corrcoef(detection_history, fps_history)[0, 1]\n",
    "        axes[1, 1].text(0.05, 0.95, f'Correlation: {correlation:.3f}',\n",
    "                       transform=axes[1, 1].transAxes,\n",
    "                       verticalalignment='top',\n",
    "                       bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "        \n",
    "        plt.suptitle('Video Inference Performance Analysis', fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def live_camera_demo(self, camera_index=0, duration_sec=10):\n",
    "        \"\"\"Live camera inference demo.\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"LIVE CAMERA INFERENCE DEMO\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Duration: {duration_sec} seconds\")\n",
    "        print(\"Press 'q' to quit early\")\n",
    "        \n",
    "        # Open camera\n",
    "        cap = cv2.VideoCapture(camera_index)\n",
    "        if not cap.isOpened():\n",
    "            print(f\"Error opening camera {camera_index}\")\n",
    "            print(\"Using synthetic camera feed instead...\")\n",
    "            return self.synthetic_camera_demo(duration_sec)\n",
    "        \n",
    "        # Get camera properties\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        \n",
    "        print(f\"Camera: {width}x{height}, {fps:.1f} FPS\")\n",
    "        \n",
    "        # Create window\n",
    "        cv2.namedWindow('Live Inference', cv2.WINDOW_NORMAL)\n",
    "        \n",
    "        # Statistics\n",
    "        frame_count = 0\n",
    "        start_time = time.time()\n",
    "        fps_history = []\n",
    "        \n",
    "        print(\"\\nStarting live inference...\")\n",
    "        \n",
    "        while time.time() - start_time < duration_sec:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                print(\"Error reading frame from camera\")\n",
    "                break\n",
    "            \n",
    "            # Convert BGR to RGB\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Run inference\n",
    "            frame_start = time.time()\n",
    "            \n",
    "            input_tensor = self.engine.preprocessor(frame_rgb)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                if self.config['inference']['use_amp']:\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        outputs = self.engine.engine.infer(input_tensor)\n",
    "                else:\n",
    "                    outputs = self.engine.engine.infer(input_tensor)\n",
    "            \n",
    "            inference_time = (time.time() - frame_start) * 1000\n",
    "            \n",
    "            # Postprocess\n",
    "            detections = self.engine.postprocessor(outputs)\n",
    "            \n",
    "            # Visualize\n",
    "            visualized = self.visualizer.visualize_detections(\n",
    "                frame_rgb,\n",
    "                detections,\n",
    "                show_confidence=True,\n",
    "                show_class_names=True\n",
    "            )\n",
    "            \n",
    "            # Convert back to BGR for display\n",
    "            visualized_bgr = cv2.cvtColor(visualized, cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "            # Add overlay info\n",
    "            current_fps = 1000 / inference_time if inference_time > 0 else 0\n",
    "            fps_history.append(current_fps)\n",
    "            \n",
    "            overlay_text = [\n",
    "                f\"FPS: {current_fps:.1f}\",\n",
    "                f\"Detections: {len(detections['boxes'])}\",\n",
    "                f\"Frame: {frame_count}\",\n",
    "                f\"Time: {time.time() - start_time:.1f}s\"\n",
    "            ]\n",
    "            \n",
    "            y_offset = 30\n",
    "            for text in overlay_text:\n",
    "                cv2.putText(visualized_bgr, text, (10, y_offset),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "                y_offset += 30\n",
    "            \n",
    "            # Display\n",
    "            cv2.imshow('Live Inference', visualized_bgr)\n",
    "            \n",
    "            frame_count += 1\n",
    "            \n",
    "            # Check for quit\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                print(\"\\nUser requested quit\")\n",
    "                break\n",
    "        \n",
    "        # Cleanup\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        \n",
    "        # Performance summary\n",
    "        total_time = time.time() - start_time\n",
    "        avg_fps = np.mean(fps_history) if fps_history else 0\n",
    "        \n",
    "        print(f\"\\nLive demo completed!\")\n",
    "        print(f\"Frames processed: {frame_count}\")\n",
    "        print(f\"Total time: {total_time:.1f} seconds\")\n",
    "        print(f\"Average FPS: {avg_fps:.1f}\")\n",
    "        \n",
    "        return fps_history\n",
    "    \n",
    "    def synthetic_camera_demo(self, duration_sec=10):\n",
    "        \"\"\"Synthetic camera demo.\"\"\"\n",
    "        print(\"Running synthetic camera demo...\")\n",
    "        \n",
    "        fps_history = []\n",
    "        start_time = time.time()\n",
    "        frame_count = 0\n",
    "        \n",
    "        cv2.namedWindow('Synthetic Camera', cv2.WINDOW_NORMAL)\n",
    "        \n",
    "        while time.time() - start_time < duration_sec:\n",
    "            # Create synthetic frame\n",
    "            frame = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)\n",
    "            \n",
    "            # Add moving object\n",
    "            t = time.time() - start_time\n",
    "            x = int(320 + 200 * np.sin(t * 2))\n",
    "            y = int(240 + 150 * np.cos(t * 1.5))\n",
    "            cv2.circle(frame, (x, y), 50, (0, 255, 0), -1)\n",
    "            \n",
    "            # Add static objects\n",
    "            cv2.rectangle(frame, (100, 100), (200, 200), (255, 0, 0), -1)\n",
    "            cv2.circle(frame, (500, 300), 40, (0, 0, 255), -1)\n",
    "            \n",
    "            # Simulate inference time\n",
    "            time.sleep(0.033)  # ~30 FPS\n",
    "            \n",
    "            # Add overlay\n",
    "            current_time = time.time() - start_time\n",
    "            current_fps = 30  # Simulated\n",
    "            \n",
    "            fps_history.append(current_fps)\n",
    "            \n",
    "            overlay_text = [\n",
    "                \"SYNTHETIC CAMERA DEMO\",\n",
    "                f\"FPS: {current_fps:.1f}\",\n",
    "                f\"Time: {current_time:.1f}s\",\n",
    "                f\"Frame: {frame_count}\",\n",
    "                \"Press 'q' to quit\"\n",
    "            ]\n",
    "            \n",
    "            y_offset = 30\n",
    "            for text in overlay_text:\n",
    "                cv2.putText(frame, text, (10, y_offset),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "                y_offset += 30\n",
    "            \n",
    "            cv2.imshow('Synthetic Camera', frame)\n",
    "            \n",
    "            frame_count += 1\n",
    "            \n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "        \n",
    "        cv2.destroyAllWindows()\n",
    "        \n",
    "        print(f\"\\nSynthetic demo completed!\")\n",
    "        print(f\"Frames: {frame_count}\")\n",
    "        print(f\"Average FPS: {np.mean(fps_history):.1f}\")\n",
    "        \n",
    "        return fps_history\n",
    "\n",
    "# %%\n",
    "# Run video inference demo\n",
    "video_demo = VideoInferenceDemo(inference_demo, config)\n",
    "\n",
    "# Process video file (if available)\n",
    "video_path = '../data/samples/sample_video.mp4'  # Example path\n",
    "if os.path.exists(video_path):\n",
    "    processed_frames = video_demo.process_video_file(\n",
    "        video_path, \n",
    "        max_frames=50,\n",
    "        output_path='../results/inference/output_video.mp4'\n",
    "    )\n",
    "else:\n",
    "    print(f\"\\nVideo file not found: {video_path}\")\n",
    "    print(\"Running synthetic video demo instead...\")\n",
    "    processed_frames = video_demo.create_synthetic_video(num_frames=50)\n",
    "\n",
    "# Live camera demo (optional)\n",
    "run_live_demo = False  # Set to True to run live demo\n",
    "if run_live_demo:\n",
    "    print(\"\\nStarting live camera demo...\")\n",
    "    fps_history = video_demo.live_camera_demo(duration_sec=5)\n",
    "else:\n",
    "    print(\"\\nSkipping live camera demo (set run_live_demo=True to enable)\")\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 6. Deployment Readiness Test\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "class DeploymentTester:\n",
    "    \"\"\"Test deployment readiness.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, config):\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "        self.device = torch.device(config['inference']['device'])\n",
    "        \n",
    "    def run_comprehensive_tests(self):\n",
    "        \"\"\"Run comprehensive deployment tests.\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"DEPLOYMENT READINESS TESTS\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        test_results = []\n",
    "        \n",
    "        # Test 1: Model loading and validation\n",
    "        test_results.append(self.test_model_loading())\n",
    "        \n",
    "        # Test 2: Inference correctness\n",
    "        test_results.append(self.test_inference_correctness())\n",
    "        \n",
    "        # Test 3: Performance requirements\n",
    "        test_results.append(self.test_performance_requirements())\n",
    "        \n",
    "        # Test 4: Memory constraints\n",
    "        test_results.append(self.test_memory_constraints())\n",
    "        \n",
    "        # Test 5: Robustness\n",
    "        test_results.append(self.test_robustness())\n",
    "        \n",
    "        # Test 6: Export capabilities\n",
    "        test_results.append(self.test_export_capabilities())\n",
    "        \n",
    "        # Generate overall assessment\n",
    "        self.generate_deployment_assessment(test_results)\n",
    "        \n",
    "        return test_results\n",
    "    \n",
    "    def test_model_loading(self):\n",
    "        \"\"\"Test model loading and validation.\"\"\"\n",
    "        print(\"\\n[TEST 1] Model Loading and Validation:\")\n",
    "        \n",
    "        results = {\n",
    "            'name': 'Model Loading',\n",
    "            'status': 'PASS',\n",
    "            'details': [],\n",
    "            'recommendations': []\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Check model is in eval mode\n",
    "            if self.model.training:\n",
    "                results['status'] = 'FAIL'\n",
    "                results['details'].append('Model is in training mode')\n",
    "                results['recommendations'].append('Set model to eval mode: model.eval()')\n",
    "            else:\n",
    "                results['details'].append('✅ Model is in evaluation mode')\n",
    "            \n",
    "            # Check device placement\n",
    "            if next(self.model.parameters()).device != self.device:\n",
    "                results['status'] = 'FAIL'\n",
    "                results['details'].append(f'Model not on correct device. Expected: {self.device}, Got: {next(self.model.parameters()).device}')\n",
    "                results['recommendations'].append(f'Move model to device: model.to({self.device})')\n",
    "            else:\n",
    "                results['details'].append(f'✅ Model correctly placed on {self.device}')\n",
    "            \n",
    "            # Check parameter count\n",
    "            total_params = sum(p.numel() for p in self.model.parameters())\n",
    "            results['details'].append(f'✅ Total parameters: {total_params:,}')\n",
    "            \n",
    "            # Test forward pass\n",
    "            H, W = self.config['model']['image_size']\n",
    "            test_input = torch.randn(1, 3, H, W).to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                if self.config['inference']['use_amp']:\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        output = self.model(test_input, task='detection')\n",
    "                else:\n",
    "                    output = self.model(test_input, task='detection')\n",
    "            \n",
    "            if isinstance(output, dict) and 'detections' in output:\n",
    "                results['details'].append('✅ Forward pass successful with detection output')\n",
    "            else:\n",
    "                results['details'].append('✅ Forward pass successful')\n",
    "            \n",
    "            print(f\"  Status: {results['status']}\")\n",
    "            for detail in results['details']:\n",
    "                print(f\"    {detail}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            results['status'] = 'FAIL'\n",
    "            results['details'].append(f'❌ Error during test: {str(e)}')\n",
    "            print(f\"  Status: {results['status']}\")\n",
    "            print(f\"  Error: {str(e)}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def test_inference_correctness(self):\n",
    "        \"\"\"Test inference correctness.\"\"\"\n",
    "        print(\"\\n[TEST 2] Inference Correctness:\")\n",
    "        \n",
    "        results = {\n",
    "            'name': 'Inference Correctness',\n",
    "            'status': 'PASS',\n",
    "            'details': [],\n",
    "            'recommendations': []\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Create test images with known objects\n",
    "            test_cases = self.create_test_cases()\n",
    "            \n",
    "            correct_predictions = 0\n",
    "            total_predictions = 0\n",
    "            \n",
    "            for i, (image, expected_classes) in enumerate(test_cases):\n",
    "                # Run inference\n",
    "                input_tensor = torch.from_numpy(image).permute(2, 0, 1).unsqueeze(0).float().to(self.device)\n",
    "                input_tensor = input_tensor / 255.0  # Normalize\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    if self.config['inference']['use_amp']:\n",
    "                        with torch.cuda.amp.autocast():\n",
    "                            output = self.model(input_tensor, task='detection')\n",
    "                    else:\n",
    "                        output = self.model(input_tensor, task='detection')\n",
    "                \n",
    "                # Simple correctness check (for demo purposes)\n",
    "                # In real testing, would compare with ground truth\n",
    "                if isinstance(output, dict) and 'detections' in output:\n",
    "                    detections = output['detections']\n",
    "                    # Count as correct if we get some detections\n",
    "                    if detections.shape[1] > 0:  # Has some detections\n",
    "                        correct_predictions += 1\n",
    "                \n",
    "                total_predictions += 1\n",
    "            \n",
    "            accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "            \n",
    "            if accuracy >= 0.8:\n",
    "                results['details'].append(f'✅ Inference accuracy: {accuracy:.1%}')\n",
    "            elif accuracy >= 0.5:\n",
    "                results['details'].append(f'⚠️ Moderate inference accuracy: {accuracy:.1%}')\n",
    "                results['recommendations'].append('Consider fine-tuning on target domain')\n",
    "            else:\n",
    "                results['status'] = 'FAIL'\n",
    "                results['details'].append(f'❌ Low inference accuracy: {accuracy:.1%}')\n",
    "                results['recommendations'].append('Model needs significant improvement')\n",
    "            \n",
    "            results['details'].append(f'Tested on {total_predictions} synthetic images')\n",
    "            \n",
    "            print(f\"  Status: {results['status']}\")\n",
    "            for detail in results['details']:\n",
    "                print(f\"    {detail}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            results['status'] = 'FAIL'\n",
    "            results['details'].append(f'❌ Error during test: {str(e)}')\n",
    "            print(f\"  Status: {results['status']}\")\n",
    "            print(f\"  Error: {str(e)}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def create_test_cases(self):\n",
    "        \"\"\"Create test cases for correctness testing.\"\"\"\n",
    "        test_cases = []\n",
    "        \n",
    "        # Create synthetic test images\n",
    "        for i in range(5):\n",
    "            # Create image with geometric shapes\n",
    "            image = np.zeros((416, 416, 3), dtype=np.uint8)\n",
    "            \n",
    "            # Add different colored rectangles\n",
    "            colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0), (255, 0, 255)]\n",
    "            \n",
    "            for j, color in enumerate(colors[:3]):  # Add 3 objects\n",
    "                x = 50 + j * 100\n",
    "                y = 50 + i * 70\n",
    "                cv2.rectangle(image, (x, y), (x+80, y+80), color, -1)\n",
    "            \n",
    "            test_cases.append((image, [0, 1, 2]))  # Expected class IDs\n",
    "        \n",
    "        return test_cases\n",
    "    \n",
    "    def test_performance_requirements(self):\n",
    "        \"\"\"Test performance requirements.\"\"\"\n",
    "        print(\"\\n[TEST 3] Performance Requirements:\")\n",
    "        \n",
    "        results = {\n",
    "            'name': 'Performance',\n",
    "            'status': 'PASS',\n",
    "            'details': [],\n",
    "            'recommendations': []\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Requirements\n",
    "            req_latency = 50  # ms (for real-time robotics)\n",
    "            req_fps = 20      # FPS minimum\n",
    "            req_batch_latency = 100  # ms for batch size 4\n",
    "            \n",
    "            # Test single image latency\n",
    "            H, W = self.config['model']['image_size']\n",
    "            test_input = torch.randn(1, 3, H, W).to(self.device)\n",
    "            \n",
    "            # Warmup\n",
    "            for _ in range(10):\n",
    "                with torch.no_grad():\n",
    "                    _ = self.model(test_input, task='detection')\n",
    "            \n",
    "            # Benchmark\n",
    "            latencies = []\n",
    "            for _ in range(50):\n",
    "                torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "                start = time.perf_counter()\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    if self.config['inference']['use_amp']:\n",
    "                        with torch.cuda.amp.autocast():\n",
    "                            _ = self.model(test_input, task='detection')\n",
    "                    else:\n",
    "                        _ = self.model(test_input, task='detection')\n",
    "                \n",
    "                torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "                end = time.perf_counter()\n",
    "                latencies.append((end - start) * 1000)\n",
    "            \n",
    "            mean_latency = np.mean(latencies)\n",
    "            fps = 1000 / mean_latency\n",
    "            \n",
    "            # Check requirements\n",
    "            if mean_latency <= req_latency:\n",
    "                results['details'].append(f'✅ Latency: {mean_latency:.1f} ms (≤ {req_latency} ms)')\n",
    "            else:\n",
    "                results['status'] = 'WARNING'\n",
    "                results['details'].append(f'⚠️ Latency: {mean_latency:.1f} ms (> {req_latency} ms)')\n",
    "                results['recommendations'].append('Optimize model for lower latency')\n",
    "            \n",
    "            if fps >= req_fps:\n",
    "                results['details'].append(f'✅ Throughput: {fps:.1f} FPS (≥ {req_fps} FPS)')\n",
    "            else:\n",
    "                results['status'] = 'FAIL'\n",
    "                results['details'].append(f'❌ Throughput: {fps:.1f} FPS (< {req_fps} FPS)')\n",
    "                results['recommendations'].append('Improve inference speed')\n",
    "            \n",
    "            # Test batch latency\n",
    "            batch_input = torch.randn(4, 3, H, W).to(self.device)\n",
    "            \n",
    "            torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "            start = time.perf_counter()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                if self.config['inference']['use_amp']:\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        _ = self.model(batch_input, task='detection')\n",
    "                else:\n",
    "                    _ = self.model(batch_input, task='detection')\n",
    "            \n",
    "            torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "            batch_latency = (time.perf_counter() - start) * 1000\n",
    "            \n",
    "            if batch_latency <= req_batch_latency:\n",
    "                results['details'].append(f'✅ Batch (4) latency: {batch_latency:.1f} ms')\n",
    "            else:\n",
    "                results['details'].append(f'⚠️ Batch (4) latency: {batch_latency:.1f} ms')\n",
    "            \n",
    "            results['details'].append(f'Latency std: {np.std(latencies):.1f} ms')\n",
    "            \n",
    "            print(f\"  Status: {results['status']}\")\n",
    "            for detail in results['details']:\n",
    "                print(f\"    {detail}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            results['status'] = 'FAIL'\n",
    "            results['details'].append(f'❌ Error during test: {str(e)}')\n",
    "            print(f\"  Status: {results['status']}\")\n",
    "            print(f\"  Error: {str(e)}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def test_memory_constraints(self):\n",
    "        \"\"\"Test memory constraints.\"\"\"\n",
    "        print(\"\\n[TEST 4] Memory Constraints:\")\n",
    "        \n",
    "        results = {\n",
    "            'name': 'Memory',\n",
    "            'status': 'PASS',\n",
    "            'details': [],\n",
    "            'recommendations': []\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Requirements (for edge deployment)\n",
    "            req_single_image_memory = 2.0  # GB maximum for single image\n",
    "            req_model_memory = 1.0  # GB maximum for model\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                # Get GPU memory info\n",
    "                gpu = GPUtil.getGPUs()[0]\n",
    "                total_memory = gpu.memoryTotal / 1024  # Convert to GB\n",
    "                \n",
    "                # Clear cache and measure\n",
    "                torch.cuda.empty_cache()\n",
    "                torch.cuda.reset_peak_memory_stats()\n",
    "                \n",
    "                initial_memory = torch.cuda.memory_allocated() / 1024**3\n",
    "                \n",
    "                # Load model and do inference\n",
    "                H, W = self.config['model']['image_size']\n",
    "                test_input = torch.randn(1, 3, H, W).to(self.device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    if self.config['inference']['use_amp']:\n",
    "                        with torch.cuda.amp.autocast():\n",
    "                            _ = self.model(test_input, task='detection')\n",
    "                    else:\n",
    "                        _ = self.model(test_input, task='detection')\n",
    "                \n",
    "                peak_memory = torch.cuda.max_memory_allocated() / 1024**3\n",
    "                memory_used = peak_memory - initial_memory\n",
    "                \n",
    "                # Check requirements\n",
    "                if memory_used <= req_single_image_memory:\n",
    "                    results['details'].append(f'✅ Memory usage: {memory_used:.2f} GB (≤ {req_single_image_memory} GB)')\n",
    "                else:\n",
    "                    results['status'] = 'WARNING'\n",
    "                    results['details'].append(f'⚠️ Memory usage: {memory_used:.2f} GB (> {req_single_image_memory} GB)')\n",
    "                    results['recommendations'].append('Reduce model size or use memory optimization')\n",
    "                \n",
    "                # Estimate model memory\n",
    "                param_memory = sum(p.numel() * p.element_size() for p in self.model.parameters()) / 1024**3\n",
    "                \n",
    "                if param_memory <= req_model_memory:\n",
    "                    results['details'].append(f'✅ Model parameters: {param_memory:.2f} GB')\n",
    "                else:\n",
    "                    results['details'].append(f'⚠️ Model parameters: {param_memory:.2f} GB')\n",
    "                \n",
    "                results['details'].append(f'GPU memory available: {total_memory:.1f} GB')\n",
    "                results['details'].append(f'Peak memory allocated: {peak_memory:.2f} GB')\n",
    "                \n",
    "            else:\n",
    "                # CPU memory test\n",
    "                import psutil\n",
    "                process = psutil.Process()\n",
    "                \n",
    "                initial_memory = process.memory_info().rss / 1024**3\n",
    "                \n",
    "                # Do inference\n",
    "                H, W = self.config['model']['image_size']\n",
    "                test_input = torch.randn(1, 3, H, W)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    _ = self.model(test_input, task='detection')\n",
    "                \n",
    "                final_memory = process.memory_info().rss / 1024**3\n",
    "                memory_used = final_memory - initial_memory\n",
    "                \n",
    "                results['details'].append(f'CPU memory usage: {memory_used:.2f} GB')\n",
    "                results['details'].append('⚠️ GPU memory test skipped (CPU only)')\n",
    "            \n",
    "            print(f\"  Status: {results['status']}\")\n",
    "            for detail in results['details']:\n",
    "                print(f\"    {detail}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            results['status'] = 'WARNING'\n",
    "            results['details'].append(f'⚠️ Memory test incomplete: {str(e)}')\n",
    "            print(f\"  Status: {results['status']}\")\n",
    "            print(f\"  Warning: {str(e)}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def test_robustness(self):\n",
    "        \"\"\"Test model robustness.\"\"\"\n",
    "        print(\"\\n[TEST 5] Robustness:\")\n",
    "        \n",
    "        results = {\n",
    "            'name': 'Robustness',\n",
    "            'status': 'PASS',\n",
    "            'details': [],\n",
    "            'recommendations': []\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Test with different input sizes\n",
    "            test_sizes = [(320, 320), (416, 416), (512, 512), (640, 640)]\n",
    "            \n",
    "            for size in test_sizes:\n",
    "                H, W = size\n",
    "                test_input = torch.randn(1, 3, H, W).to(self.device)\n",
    "                \n",
    "                try:\n",
    "                    with torch.no_grad():\n",
    "                        if self.config['inference']['use_amp']:\n",
    "                            with torch.cuda.amp.autocast():\n",
    "                                output = self.model(test_input, task='detection')\n",
    "                        else:\n",
    "                            output = self.model(test_input, task='detection')\n",
    "                    \n",
    "                    results['details'].append(f'✅ Input size {H}x{W}: Successful')\n",
    "                except Exception as e:\n",
    "                    results['status'] = 'WARNING'\n",
    "                    results['details'].append(f'⚠️ Input size {H}x{W}: Failed - {str(e)}')\n",
    "            \n",
    "            # Test with extreme values\n",
    "            extreme_inputs = [\n",
    "                torch.zeros(1, 3, 416, 416).to(self.device),  # All zeros\n",
    "                torch.ones(1, 3, 416, 416).to(self.device),   # All ones\n",
    "                torch.randn(1, 3, 416, 416).to(self.device) * 10,  # High variance\n",
    "            ]\n",
    "            \n",
    "            for i, inp in enumerate(extreme_inputs):\n",
    "                try:\n",
    "                    with torch.no_grad():\n",
    "                        output = self.model(inp, task='detection')\n",
    "                    \n",
    "                    if not torch.any(torch.isnan(output)):\n",
    "                        results['details'].append(f'✅ Extreme input {i+1}: Stable')\n",
    "                    else:\n",
    "                        results['status'] = 'WARNING'\n",
    "                        results['details'].append(f'⚠️ Extreme input {i+1}: NaN detected')\n",
    "                except Exception as e:\n",
    "                    results['status'] = 'WARNING'\n",
    "                    results['details'].append(f'⚠️ Extreme input {i+1}: Failed')\n",
    "            \n",
    "            print(f\"  Status: {results['status']}\")\n",
    "            for detail in results['details'][:5]:  # Show first 5 details\n",
    "                print(f\"    {detail}\")\n",
    "            if len(results['details']) > 5:\n",
    "                print(f\"    ... and {len(results['details']) - 5} more\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            results['status'] = 'WARNING'\n",
    "            results['details'].append(f'⚠️ Robustness test incomplete: {str(e)}')\n",
    "            print(f\"  Status: {results['status']}\")\n",
    "            print(f\"  Warning: {str(e)}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def test_export_capabilities(self):\n",
    "        \"\"\"Test model export capabilities.\"\"\"\n",
    "        print(\"\\n[TEST 6] Export Capabilities:\")\n",
    "        \n",
    "        results = {\n",
    "            'name': 'Export',\n",
    "            'status': 'PASS',\n",
    "            'details': [],\n",
    "            'recommendations': []\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Test TorchScript export\n",
    "            H, W = self.config['model']['image_size']\n",
    "            example_input = torch.randn(1, 3, H, W).to(self.device)\n",
    "            \n",
    "            try:\n",
    "                traced_model = torch.jit.trace(self.model, example_input)\n",
    "                results['details'].append('✅ TorchScript: Export successful')\n",
    "                \n",
    "                # Test inference with traced model\n",
    "                with torch.no_grad():\n",
    "                    traced_output = traced_model(example_input, task='detection')\n",
    "                results['details'].append('✅ TorchScript: Inference successful')\n",
    "            except Exception as e:\n",
    "                results['status'] = 'WARNING'\n",
    "                results['details'].append(f'⚠️ TorchScript: Export failed - {str(e)}')\n",
    "                results['recommendations'].append('Fix model for TorchScript compatibility')\n",
    "            \n",
    "            # Test ONNX export (if available)\n",
    "            try:\n",
    "                import onnx\n",
    "                import onnxruntime\n",
    "                \n",
    "                onnx_path = '../results/inference/model.onnx'\n",
    "                \n",
    "                torch.onnx.export(\n",
    "                    self.model,\n",
    "                    example_input,\n",
    "                    onnx_path,\n",
    "                    opset_version=11,\n",
    "                    input_names=['input'],\n",
    "                    output_names=['output'],\n",
    "                    dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n",
    "                )\n",
    "                \n",
    "                results['details'].append('✅ ONNX: Export successful')\n",
    "                \n",
    "                # Validate ONNX model\n",
    "                onnx_model = onnx.load(onnx_path)\n",
    "                onnx.checker.check_model(onnx_model)\n",
    "                results['details'].append('✅ ONNX: Model validation passed')\n",
    "                \n",
    "            except ImportError:\n",
    "                results['details'].append('ℹ️ ONNX: Package not available')\n",
    "            except Exception as e:\n",
    "                results['details'].append(f'⚠️ ONNX: Export failed - {str(e)}')\n",
    "            \n",
    "            print(f\"  Status: {results['status']}\")\n",
    "            for detail in results['details']:\n",
    "                print(f\"    {detail}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            results['status'] = 'WARNING'\n",
    "            results['details'].append(f'⚠️ Export test incomplete: {str(e)}')\n",
    "            print(f\"  Status: {results['status']}\")\n",
    "            print(f\"  Warning: {str(e)}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def generate_deployment_assessment(self, test_results):\n",
    "        \"\"\"Generate overall deployment assessment.\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"DEPLOYMENT ASSESSMENT\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Count results\n",
    "        status_counts = {'PASS': 0, 'WARNING': 0, 'FAIL': 0}\n",
    "        all_recommendations = []\n",
    "        \n",
    "        for result in test_results:\n",
    "            status_counts[result['status']] += 1\n",
    "            all_recommendations.extend(result['recommendations'])\n",
    "        \n",
    "        # Overall status\n",
    "        if status_counts['FAIL'] > 0:\n",
    "            overall_status = '❌ NOT READY'\n",
    "        elif status_counts['WARNING'] > 0:\n",
    "            overall_status = '⚠️ READY WITH WARNINGS'\n",
    "        else:\n",
    "            overall_status = '✅ READY FOR DEPLOYMENT'\n",
    "        \n",
    "        print(f\"Overall Status: {overall_status}\")\n",
    "        print(f\"\\nTest Results: PASS={status_counts['PASS']}, WARNING={status_counts['WARNING']}, FAIL={status_counts['FAIL']}\")\n",
    "        \n",
    "        # Show failed tests\n",
    "        failed_tests = [r['name'] for r in test_results if r['status'] == 'FAIL']\n",
    "        if failed_tests:\n",
    "            print(f\"\\nFailed Tests: {', '.join(failed_tests)}\")\n",
    "        \n",
    "        # Show warnings\n",
    "        warning_tests = [r['name'] for r in test_results if r['status'] == 'WARNING']\n",
    "        if warning_tests:\n",
    "            print(f\"\\nTests with Warnings: {', '.join(warning_tests)}\")\n",
    "        \n",
    "        # Recommendations\n",
    "        if all_recommendations:\n",
    "            print(f\"\\nKey Recommendations:\")\n",
    "            for i, rec in enumerate(set(all_recommendations), 1):\n",
    "                print(f\"  {i}. {rec}\")\n",
    "        \n",
    "        # Deployment readiness score\n",
    "        total_tests = len(test_results)\n",
    "        readiness_score = (status_counts['PASS'] + 0.5 * status_counts['WARNING']) / total_tests\n",
    "        \n",
    "        print(f\"\\nDeployment Readiness Score: {readiness_score:.1%}\")\n",
    "        \n",
    "        if readiness_score >= 0.9:\n",
    "            print(\"Assessment: Excellent - Ready for production deployment\")\n",
    "        elif readiness_score >= 0.7:\n",
    "            print(\"Assessment: Good - Ready with minor optimizations\")\n",
    "        elif readiness_score >= 0.5:\n",
    "            print(\"Assessment: Fair - Needs significant improvements\")\n",
    "        else:\n",
    "            print(\"Assessment: Poor - Not ready for deployment\")\n",
    "        \n",
    "        # Next steps\n",
    "        print(f\"\\nNext Steps:\")\n",
    "        if status_counts['FAIL'] > 0:\n",
    "            print(\"  1. Address all FAILED tests immediately\")\n",
    "        if status_counts['WARNING'] > 0:\n",
    "            print(\"  2. Review and address WARNING tests\")\n",
    "        print(\"  3. Run integration tests with target hardware\")\n",
    "        print(\"  4. Perform stress testing\")\n",
    "        print(\"  5. Create deployment package\")\n",
    "        \n",
    "        return overall_status, readiness_score\n",
    "\n",
    "# %%\n",
    "# Run deployment tests\n",
    "deployment_tester = DeploymentTester(model, config)\n",
    "test_results = deployment_tester.run_comprehensive_tests()\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 7. Interactive Demo with Widgets\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "class InteractiveDemo:\n",
    "    \"\"\"Interactive demo with IPython widgets.\"\"\"\n",
    "    \n",
    "    def __init__(self, inference_demo, config):\n",
    "        self.inference_demo = inference_demo\n",
    "        self.config = config\n",
    "        \n",
    "        # Create widgets\n",
    "        self.create_widgets()\n",
    "        \n",
    "    def create_widgets(self):\n",
    "        \"\"\"Create interactive widgets.\"\"\"\n",
    "        print(\"\\nCreating interactive demo...\")\n",
    "        \n",
    "        # Image upload widget\n",
    "        self.upload_widget = widgets.FileUpload(\n",
    "            accept='.jpg,.jpeg,.png',\n",
    "            multiple=False,\n",
    "            description='Upload Image'\n",
    "        )\n",
    "        \n",
    "        # Confidence threshold slider\n",
    "        self.confidence_slider = widgets.FloatSlider(\n",
    "            value=self.config['model']['confidence_threshold'],\n",
    "            min=0.0,\n",
    "            max=1.0,\n",
    "            step=0.05,\n",
    "            description='Confidence:',\n",
    "            continuous_update=False\n",
    "        )\n",
    "        \n",
    "        # IOU threshold slider\n",
    "        self.iou_slider = widgets.FloatSlider(\n",
    "            value=self.config['model']['iou_threshold'],\n",
    "            min=0.0,\n",
    "            max=1.0,\n",
    "            step=0.05,\n",
    "            description='IOU:',\n",
    "            continuous_update=False\n",
    "        )\n",
    "        \n",
    "        # Inference button\n",
    "        self.inference_button = widgets.Button(\n",
    "            description='Run Inference',\n",
    "            button_style='success',\n",
    "            icon='play'\n",
    "        )\n",
    "        \n",
    "        # Results display\n",
    "        self.results_output = widgets.Output()\n",
    "        \n",
    "        # Connect callbacks\n",
    "        self.inference_button.on_click(self.run_interactive_inference)\n",
    "        \n",
    "        # Display widgets\n",
    "        display(widgets.VBox([\n",
    "            widgets.HTML(\"<h3>Interactive Inference Demo</h3>\"),\n",
    "            self.upload_widget,\n",
    "            self.confidence_slider,\n",
    "            self.iou_slider,\n",
    "            self.inference_button,\n",
    "            self.results_output\n",
    "        ]))\n",
    "    \n",
    "    def run_interactive_inference(self, b):\n",
    "        \"\"\"Run inference with current widget settings.\"\"\"\n",
    "        with self.results_output:\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            # Get uploaded image\n",
    "            if not self.upload_widget.value:\n",
    "                print(\"Please upload an image first!\")\n",
    "                return\n",
    "            \n",
    "            # Get the uploaded file\n",
    "            uploaded_file = list(self.upload_widget.value.values())[0]\n",
    "            \n",
    "            # Read image\n",
    "            import io\n",
    "            image_data = uploaded_file['content']\n",
    "            image = Image.open(io.BytesIO(image_data))\n",
    "            image_np = np.array(image)\n",
    "            \n",
    "            print(f\"Image loaded: {image.size}\")\n",
    "            \n",
    "            # Update thresholds\n",
    "            self.inference_demo.engine.confidence_threshold = self.confidence_slider.value\n",
    "            self.inference_demo.engine.iou_threshold = self.iou_slider.value\n",
    "            \n",
    "            # Run inference\n",
    "            start_time = time.time()\n",
    "            \n",
    "            input_tensor = self.inference_demo.preprocessor(image_np)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                if self.config['inference']['use_amp']:\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        outputs = self.inference_demo.engine.infer(input_tensor)\n",
    "                else:\n",
    "                    outputs = self.inference_demo.engine.infer(input_tensor)\n",
    "            \n",
    "            inference_time = (time.time() - start_time) * 1000\n",
    "            \n",
    "            # Postprocess with updated thresholds\n",
    "            self.inference_demo.postprocessor.confidence_threshold = self.confidence_slider.value\n",
    "            self.inference_demo.postprocessor.iou_threshold = self.iou_slider.value\n",
    "            \n",
    "            detections = self.inference_demo.postprocessor(outputs)\n",
    "            \n",
    "            # Visualize\n",
    "            visualized = self.inference_demo.visualizer.visualize_detections(\n",
    "                image_np,\n",
    "                detections,\n",
    "                show_confidence=True,\n",
    "                show_class_names=True\n",
    "            )\n",
    "            \n",
    "            # Display results\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "            \n",
    "            axes[0].imshow(image_np)\n",
    "            axes[0].set_title('Original Image')\n",
    "            axes[0].axis('off')\n",
    "            \n",
    "            axes[1].imshow(visualized)\n",
    "            axes[1].set_title(f'Detections ({len(detections[\"boxes\"])} objects)')\n",
    "            axes[1].axis('off')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Print statistics\n",
    "            print(f\"\\nInference Results:\")\n",
    "            print(f\"  Inference time: {inference_time:.2f} ms\")\n",
    "            print(f\"  FPS: {1000/inference_time:.1f}\")\n",
    "            print(f\"  Detected objects: {len(detections['boxes'])}\")\n",
    "            print(f\"  Confidence threshold: {self.confidence_slider.value}\")\n",
    "            print(f\"  IOU threshold: {self.iou_slider.value}\")\n",
    "            \n",
    "            if len(detections['boxes']) > 0:\n",
    "                print(f\"\\nTop detections:\")\n",
    "                for i in range(min(3, len(detections['boxes']))):\n",
    "                    class_name = detections['class_names'][i]\n",
    "                    confidence = detections['scores'][i]\n",
    "                    print(f\"  {i+1}. {class_name}: {confidence:.3f}\")\n",
    "\n",
    "# %%\n",
    "# Create interactive demo\n",
    "try:\n",
    "    interactive_demo = InteractiveDemo(inference_demo, config)\n",
    "except Exception as e:\n",
    "    print(f\"Interactive demo setup failed: {e}\")\n",
    "    print(\"Continuing with other tests...\")\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 8. Export Inference Analysis Report\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "class InferenceAnalysisExporter:\n",
    "    \"\"\"Export comprehensive inference analysis report.\"\"\"\n",
    "    \n",
    "    def __init__(self, config, benchmark_results, deployment_results):\n",
    "        self.config = config\n",
    "        self.benchmark_results = benchmark_results\n",
    "        self.deployment_results = deployment_results\n",
    "        \n",
    "    def export_report(self):\n",
    "        \"\"\"Export inference analysis report.\"\"\"\n",
    "        print(\"\\nExporting Inference Analysis Report...\")\n",
    "        \n",
    "        # Create report data\n",
    "        report = {\n",
    "            'timestamp': pd.Timestamp.now().isoformat(),\n",
    "            'config': self.config,\n",
    "            'performance_summary': self.generate_performance_summary(),\n",
    "            'benchmark_results': self.benchmark_results,\n",
    "            'deployment_assessment': self.analyze_deployment_results(),\n",
    "            'recommendations': self.generate_inference_recommendations(),\n",
    "            'next_steps': self.generate_next_steps()\n",
    "        }\n",
    "        \n",
    "        # Export as JSON\n",
    "        import json\n",
    "        with open('../reports/inference_analysis_report.json', 'w') as f:\n",
    "            json.dump(report, f, indent=2)\n",
    "        \n",
    "        print(\"Inference analysis report exported to ../reports/inference_analysis_report.json\")\n",
    "        \n",
    "        # Also export as HTML\n",
    "        self.export_html_report(report)\n",
    "    \n",
    "    def generate_performance_summary(self):\n",
    "        \"\"\"Generate performance summary.\"\"\"\n",
    "        if 'latencies' not in locals():\n",
    "            return \"Performance benchmarks not run.\"\n",
    "        \n",
    "        summary = f\"\"\"\n",
    "        INFERENCE PERFORMANCE SUMMARY\n",
    "        \n",
    "        Hardware:\n",
    "        - Device: {self.config['inference']['device']}\n",
    "        - Mixed precision: {self.config['inference']['use_amp']}\n",
    "        \n",
    "        Single Image Inference:\n",
    "        - Mean latency: {np.mean(latencies):.2f} ms\n",
    "        - Throughput: {1000/np.mean(latencies):.1f} FPS\n",
    "        - Latency std: {np.std(latencies):.2f} ms\n",
    "        \n",
    "        Memory Usage:\n",
    "        - Single image: {memory_results[0][1] if memory_results else 'N/A':.3f} GB\n",
    "        - Batch efficiency: {np.mean([mem/bs for bs, mem in memory_results]) if memory_results else 'N/A':.3f} GB per image\n",
    "        \n",
    "        Optimal Configuration:\n",
    "        - Batch size: {optimal['batch_size'] if 'optimal' in locals() else 'N/A'}\n",
    "        - Max throughput: {optimal['throughput'] if 'optimal' in locals() else 'N/A':.1f} images/sec\n",
    "        \n",
    "        Real-time Capability: {'✅ READY' if np.mean(latencies) < 50 else '⚠️ NEEDS OPTIMIZATION'}\n",
    "        \"\"\"\n",
    "        return summary\n",
    "    \n",
    "    def analyze_deployment_results(self):\n",
    "        \"\"\"Analyze deployment test results.\"\"\"\n",
    "        if not hasattr(self, 'deployment_results'):\n",
    "            return \"Deployment tests not run.\"\n",
    "        \n",
    "        # Count results\n",
    "        status_counts = {'PASS': 0, 'WARNING': 0, 'FAIL': 0}\n",
    "        for result in self.deployment_results:\n",
    "            status_counts[result['status']] += 1\n",
    "        \n",
    "        assessment = {\n",
    "            'total_tests': len(self.deployment_results),\n",
    "            'passed': status_counts['PASS'],\n",
    "            'warnings': status_counts['WARNING'],\n",
    "            'failed': status_counts['FAIL'],\n",
    "            'readiness_score': (status_counts['PASS'] + 0.5 * status_counts['WARNING']) / len(self.deployment_results),\n",
    "            'overall_status': 'READY' if status_counts['FAIL'] == 0 else 'NOT READY'\n",
    "        }\n",
    "        \n",
    "        return assessment\n",
    "    \n",
    "    def generate_inference_recommendations(self):\n",
    "        \"\"\"Generate inference optimization recommendations.\"\"\"\n",
    "        recommendations = []\n",
    "        \n",
    "        # Latency recommendations\n",
    "        if 'latencies' in locals() and np.mean(latencies) > 50:\n",
    "            recommendations.append({\n",
    "                'category': 'Performance',\n",
    "                'issue': 'High inference latency',\n",
    "                'recommendation': 'Optimize model architecture or use TensorRT',\n",
    "                'priority': 'HIGH'\n",
    "            })\n",
    "        \n",
    "        # Memory recommendations\n",
    "        if memory_results and memory_results[0][1] > 2.0:  # More than 2GB for single image\n",
    "            recommendations.append({\n",
    "                'category': 'Memory',\n",
    "                'issue': 'High memory usage',\n",
    "                'recommendation': 'Implement model pruning or quantization',\n",
    "                'priority': 'HIGH'\n",
    "            })\n",
    "        \n",
    "        # Batch size recommendations\n",
    "        if 'optimal' in locals() and optimal['batch_size'] > 8:\n",
    "            recommendations.append({\n",
    "                'category': 'Throughput',\n",
    "                'issue': 'Large optimal batch size',\n",
    "                'recommendation': f'Use batch size {optimal[\"batch_size\"]} for maximum throughput',\n",
    "                'priority': 'MEDIUM'\n",
    "            })\n",
    "        \n",
    "        # Deployment recommendations\n",
    "        if hasattr(self, 'deployment_results'):\n",
    "            for result in self.deployment_results:\n",
    "                if result['status'] == 'FAIL':\n",
    "                    recommendations.append({\n",
    "                        'category': 'Deployment',\n",
    "                        'issue': f'Failed test: {result[\"name\"]}',\n",
    "                        'recommendation': result['recommendations'][0] if result['recommendations'] else 'Fix the issue',\n",
    "                        'priority': 'HIGH'\n",
    "                    })\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def generate_next_steps(self):\n",
    "        \"\"\"Generate next steps for deployment.\"\"\"\n",
    "        next_steps = [\n",
    "            '1. Address HIGH priority recommendations',\n",
    "            '2. Run integration tests on target hardware',\n",
    "            '3. Optimize for specific deployment scenario',\n",
    "            '4. Create production deployment pipeline',\n",
    "            '5. Set up monitoring and logging',\n",
    "            '6. Perform A/B testing in staging',\n",
    "            '7. Deploy to production with gradual rollout'\n",
    "        ]\n",
    "        return next_steps\n",
    "    \n",
    "    def export_html_report(self, report):\n",
    "        \"\"\"Export HTML report.\"\"\"\n",
    "        html_content = f\"\"\"\n",
    "        <!DOCTYPE html>\n",
    "        <html>\n",
    "        <head>\n",
    "            <title>Humanoid Vision System - Inference Analysis Report</title>\n",
    "            <style>\n",
    "                body {{ font-family: Arial, sans-serif; margin: 40px; line-height: 1.6; }}\n",
    "                h1 {{ color: #2c3e50; border-bottom: 3px solid #3498db; }}\n",
    "                h2 {{ color: #34495e; margin-top: 30px; }}\n",
    "                h3 {{ color: #2c3e50; margin-top: 20px; }}\n",
    "                .card {{ background: #f8f9fa; border-left: 4px solid #3498db; \n",
    "                        padding: 20px; margin: 20px 0; border-radius: 5px; }}\n",
    "                .metric-card {{ display: inline-block; background: white; padding: 15px; \n",
    "                         margin: 10px; border-radius: 5px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); \n",
    "                         width: 200px; vertical-align: top; }}\n",
    "                .pass {{ color: #27ae60; font-weight: bold; }}\n",
    "                .warning {{ color: #f39c12; font-weight: bold; }}\n",
    "                .fail {{ color: #e74c3c; font-weight: bold; }}\n",
    "                table {{ width: 100%; border-collapse: collapse; margin: 20px 0; }}\n",
    "                th, td {{ padding: 12px; text-align: left; border-bottom: 1px solid #ddd; }}\n",
    "                th {{ background-color: #3498db; color: white; }}\n",
    "                .high {{ background: #ffeaa7; padding: 10px; border-radius: 5px; }}\n",
    "                .medium {{ background: #a29bfe; padding: 10px; border-radius: 5px; }}\n",
    "                .low {{ background: #55efc4; padding: 10px; border-radius: 5px; }}\n",
    "            </style>\n",
    "        </head>\n",
    "        <body>\n",
    "            <h1>Humanoid Vision System - Inference Analysis Report</h1>\n",
    "            <p>Generated on: {report['timestamp']}</p>\n",
    "            \n",
    "            <div class=\"card\">\n",
    "                <h2>Executive Summary</h2>\n",
    "                <pre>{report['performance_summary']}</pre>\n",
    "            </div>\n",
    "            \n",
    "            <h2>Deployment Assessment</h2>\n",
    "            <div class=\"card\">\n",
    "        \"\"\"\n",
    "        \n",
    "        if isinstance(report['deployment_assessment'], dict):\n",
    "            assessment = report['deployment_assessment']\n",
    "            html_content += f\"\"\"\n",
    "                <p><strong>Overall Status:</strong> <span class=\"{assessment['overall_status'].lower().replace(' ', '-')}\">{assessment['overall_status']}</span></p>\n",
    "                <p><strong>Readiness Score:</strong> {assessment['readiness_score']:.1%}</p>\n",
    "                <p><strong>Tests Passed:</strong> {assessment['passed']}/{assessment['total_tests']}</p>\n",
    "                <p><strong>Tests with Warnings:</strong> {assessment['warnings']}</p>\n",
    "                <p><strong>Tests Failed:</strong> {assessment['failed']}</p>\n",
    "            \"\"\"\n",
    "        else:\n",
    "            html_content += f\"<p>{report['deployment_assessment']}</p>\"\n",
    "        \n",
    "        html_content += \"\"\"\n",
    "            </div>\n",
    "            \n",
    "            <h2>Optimization Recommendations</h2>\n",
    "            <table>\n",
    "                <tr>\n",
    "                    <th>Category</th>\n",
    "                    <th>Priority</th>\n",
    "                    <th>Issue</th>\n",
    "                    <th>Recommendation</th>\n",
    "                </tr>\n",
    "        \"\"\"\n",
    "        \n",
    "        for rec in report['recommendations']:\n",
    "            priority_class = rec['priority'].lower()\n",
    "            html_content += f\"\"\"\n",
    "                <tr class=\"{priority_class}\">\n",
    "                    <td>{rec['category']}</td>\n",
    "                    <td>{rec['priority']}</td>\n",
    "                    <td>{rec['issue']}</td>\n",
    "                    <td>{rec['recommendation']}</td>\n",
    "                </tr>\n",
    "            \"\"\"\n",
    "        \n",
    "        html_content += \"\"\"\n",
    "            </table>\n",
    "            \n",
    "            <h2>Next Steps for Deployment</h2>\n",
    "            <div class=\"card\">\n",
    "                <ol>\n",
    "        \"\"\"\n",
    "        \n",
    "        for step in report['next_steps']:\n",
    "            html_content += f\"<li>{step}</li>\"\n",
    "        \n",
    "        html_content += \"\"\"\n",
    "                </ol>\n",
    "            </div>\n",
    "            \n",
    "            <div class=\"card\">\n",
    "                <h2>Key Metrics</h2>\n",
    "                <div>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Add key metrics cards\n",
    "        if 'latencies' in locals():\n",
    "            html_content += f\"\"\"\n",
    "                <div class=\"metric-card\">\n",
    "                    <h3>Latency</h3>\n",
    "                    <p>{np.mean(latencies):.1f} ms</p>\n",
    "                    <p>±{np.std(latencies):.1f} ms</p>\n",
    "                </div>\n",
    "                <div class=\"metric-card\">\n",
    "                    <h3>Throughput</h3>\n",
    "                    <p>{1000/np.mean(latencies):.1f} FPS</p>\n",
    "                </div>\n",
    "            \"\"\"\n",
    "        \n",
    "        if memory_results:\n",
    "            html_content += f\"\"\"\n",
    "                <div class=\"metric-card\">\n",
    "                    <h3>Memory</h3>\n",
    "                    <p>{memory_results[0][1]:.2f} GB</p>\n",
    "                    <p>per image</p>\n",
    "                </div>\n",
    "            \"\"\"\n",
    "        \n",
    "        if 'optimal' in locals():\n",
    "            html_content += f\"\"\"\n",
    "                <div class=\"metric-card\">\n",
    "                    <h3>Optimal Batch</h3>\n",
    "                    <p>{optimal['batch_size']}</p>\n",
    "                    <p>{optimal['throughput']:.0f} img/sec</p>\n",
    "                </div>\n",
    "            \"\"\"\n",
    "        \n",
    "        html_content += \"\"\"\n",
    "                </div>\n",
    "            </div>\n",
    "            \n",
    "            <div class=\"card\">\n",
    "                <h2>Conclusion</h2>\n",
    "                <p>The inference pipeline is {'ready for deployment' if report.get('deployment_assessment', {}).get('overall_status') == 'READY' else 'not yet ready for deployment'}. \n",
    "                {'All critical tests have passed and performance meets requirements.' if report.get('deployment_assessment', {}).get('overall_status') == 'READY' else 'Critical issues need to be addressed before deployment.'}</p>\n",
    "                <p>Proceed to deployment testing (05_deployment_test.ipynb) for the final validation.</p>\n",
    "            </div>\n",
    "        </body>\n",
    "        </html>\n",
    "        \"\"\"\n",
    "        \n",
    "        with open('../reports/inference_analysis_report.html', 'w') as f:\n",
    "            f.write(html_content)\n",
    "        \n",
    "        print(\"HTML report exported to ../reports/inference_analysis_report.html\")\n",
    "\n",
    "# %%\n",
    "# Export inference analysis report\n",
    "inference_exporter = InferenceAnalysisExporter(config, \n",
    "                                             benchmark_results=batch_results if 'batch_results' in locals() else None,\n",
    "                                             deployment_results=test_results)\n",
    "inference_exporter.export_report()\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 9. Conclusion\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INFERENCE DEMO AND ANALYSIS - COMPLETED\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n✅ DEMONSTRATIONS COMPLETED:\")\n",
    "print(\"  1. Model loading and validation\")\n",
    "print(\"  2. Single image inference\")\n",
    "print(\"  3. Batch inference\")\n",
    "print(\"  4. Performance benchmarking\")\n",
    "print(\"  5. Video processing\")\n",
    "print(\"  6. Deployment readiness tests\")\n",
    "print(\"  7. Interactive demo\")\n",
    "\n",
    "print(\"\\n📊 KEY PERFORMANCE METRICS:\")\n",
    "if 'latencies' in locals():\n",
    "    print(f\"  • Inference latency: {np.mean(latencies):.1f} ms ({1000/np.mean(latencies):.1f} FPS)\")\n",
    "if memory_results:\n",
    "    print(f\"  • Memory usage: {memory_results[0][1]:.2f} GB per image\")\n",
    "if 'optimal' in locals():\n",
    "    print(f\"  • Optimal batch size: {optimal['batch_size']} ({optimal['throughput']:.0f} images/sec)\")\n",
    "\n",
    "print(\"\\n🚀 DEPLOYMENT READINESS:\")\n",
    "if 'test_results' in locals() and test_results:\n",
    "    failed = sum(1 for r in test_results if r['status'] == 'FAIL')\n",
    "    warnings = sum(1 for r in test_results if r['status'] == 'WARNING')\n",
    "    \n",
    "    if failed == 0 and warnings == 0:\n",
    "        print(\"  ✅ EXCELLENT - Ready for production deployment\")\n",
    "    elif failed == 0:\n",
    "        print(f\"  ⚠️ GOOD - Ready with {warnings} warnings\")\n",
    "    else:\n",
    "        print(f\"  ❌ NEEDS WORK - {failed} critical issues, {warnings} warnings\")\n",
    "\n",
    "print(\"\\n🎯 NEXT STEPS:\")\n",
    "print(\"  1. Address any critical issues identified\")\n",
    "print(\"  2. Run deployment tests on target hardware\")\n",
    "print(\"  3. Create production deployment pipeline\")\n",
    "print(\"  4. Set up monitoring and alerting\")\n",
    "print(\"  5. Proceed to deployment testing (05_deployment_test.ipynb)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
