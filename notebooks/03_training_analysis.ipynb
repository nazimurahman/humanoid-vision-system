{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af24568a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "import yaml\n",
    "from typing import Dict, List, Tuple, Any, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "# Training utilities\n",
    "from src.training.mhc_trainer import ManifoldConstrainedTrainer\n",
    "from src.training.loss_functions import YOLOLoss, DetectionLoss\n",
    "from src.training.optimizer import ManifoldOptimizer\n",
    "from src.training.scheduler import CosineAnnealingWarmRestartsWithDecay\n",
    "from src.training.stability_monitor import TrainingStabilityMonitor\n",
    "\n",
    "# Data\n",
    "from src.data.dataset import COCOVisionDataset\n",
    "from src.data.transforms import VisionTransforms\n",
    "\n",
    "# Model\n",
    "from src.models.hybrid_vision import HybridVisionSystem\n",
    "\n",
    "# Visualization\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Configuration\n",
    "config = {\n",
    "    'training': {\n",
    "        'batch_size': 16,\n",
    "        'num_epochs': 100,\n",
    "        'learning_rate': 1e-3,\n",
    "        'weight_decay': 1e-4,\n",
    "        'gradient_clip': 1.0,\n",
    "        'use_amp': True,\n",
    "        'accumulation_steps': 1,\n",
    "        'warmup_epochs': 5,\n",
    "        'min_lr': 1e-6\n",
    "    },\n",
    "    'model': {\n",
    "        'input_channels': 3,\n",
    "        'base_channels': 32,\n",
    "        'num_classes': 80,\n",
    "        'image_size': (416, 416),\n",
    "        'use_vit': True,\n",
    "        'use_rag': False\n",
    "    },\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "}\n",
    "\n",
    "# Set random seed\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create logs directory\n",
    "os.makedirs('../logs/training', exist_ok=True)\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 2. Training Simulation Setup\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "class TrainingSimulator:\n",
    "    \"\"\"Simulate training for analysis purposes.\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.device = torch.device(config['device'])\n",
    "        \n",
    "        # Create model\n",
    "        self.model = self.create_model()\n",
    "        \n",
    "        # Create optimizer\n",
    "        self.optimizer = self.create_optimizer()\n",
    "        \n",
    "        # Create scheduler\n",
    "        self.scheduler = self.create_scheduler()\n",
    "        \n",
    "        # Create loss functions\n",
    "        self.loss_functions = self.create_loss_functions()\n",
    "        \n",
    "        # Training monitor\n",
    "        self.monitor = TrainingStabilityMonitor()\n",
    "        \n",
    "        # Training history\n",
    "        self.history = {\n",
    "            'losses': [],\n",
    "            'gradients': [],\n",
    "            'learning_rates': [],\n",
    "            'stability_metrics': []\n",
    "        }\n",
    "    \n",
    "    def create_model(self):\n",
    "        \"\"\"Create model for training simulation.\"\"\"\n",
    "        print(\"Creating model for training simulation...\")\n",
    "        \n",
    "        model = HybridVisionSystem(\n",
    "            config=self.config['model'],\n",
    "            num_classes=self.config['model']['num_classes'],\n",
    "            use_vit=self.config['model']['use_vit'],\n",
    "            use_rag=self.config['model']['use_rag']\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Count parameters\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        \n",
    "        print(f\"Model created:\")\n",
    "        print(f\"  Total parameters: {total_params:,}\")\n",
    "        print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def create_optimizer(self):\n",
    "        \"\"\"Create optimizer with manifold constraints.\"\"\"\n",
    "        print(\"\\nCreating ManifoldOptimizer...\")\n",
    "        \n",
    "        optimizer = ManifoldOptimizer(\n",
    "            params=self.model.parameters(),\n",
    "            lr=self.config['training']['learning_rate'],\n",
    "            weight_decay=self.config['training']['weight_decay'],\n",
    "            betas=(0.9, 0.999),\n",
    "            eps=1e-8\n",
    "        )\n",
    "        \n",
    "        print(f\"Optimizer configured:\")\n",
    "        print(f\"  Learning rate: {self.config['training']['learning_rate']}\")\n",
    "        print(f\"  Weight decay: {self.config['training']['weight_decay']}\")\n",
    "        print(f\"  Betas: (0.9, 0.999)\")\n",
    "        \n",
    "        return optimizer\n",
    "    \n",
    "    def create_scheduler(self):\n",
    "        \"\"\"Create learning rate scheduler.\"\"\"\n",
    "        print(\"\\nCreating learning rate scheduler...\")\n",
    "        \n",
    "        scheduler = CosineAnnealingWarmRestartsWithDecay(\n",
    "            optimizer=self.optimizer,\n",
    "            T_0=self.config['training']['warmup_epochs'],\n",
    "            T_mult=2,\n",
    "            eta_min=self.config['training']['min_lr'],\n",
    "            decay_factor=0.1,\n",
    "            decay_patience=10\n",
    "        )\n",
    "        \n",
    "        print(f\"Scheduler configured:\")\n",
    "        print(f\"  Warmup epochs: {self.config['training']['warmup_epochs']}\")\n",
    "        print(f\"  Minimum LR: {self.config['training']['min_lr']}\")\n",
    "        print(f\"  T_mult: 2\")\n",
    "        \n",
    "        return scheduler\n",
    "    \n",
    "    def create_loss_functions(self):\n",
    "        \"\"\"Create loss functions for different tasks.\"\"\"\n",
    "        print(\"\\nCreating loss functions...\")\n",
    "        \n",
    "        loss_functions = {\n",
    "            'detection': YOLOLoss(\n",
    "                num_classes=self.config['model']['num_classes'],\n",
    "                anchors=[[10, 13], [16, 30], [33, 23]],\n",
    "                image_size=self.config['model']['image_size']\n",
    "            ).to(self.device),\n",
    "            \n",
    "            'classification': nn.CrossEntropyLoss(),\n",
    "            \n",
    "            'regularization': nn.MSELoss()\n",
    "        }\n",
    "        \n",
    "        print(\"Loss functions created:\")\n",
    "        print(\"  - Detection: YOLO loss with 3 anchors\")\n",
    "        print(\"  - Classification: Cross-entropy\")\n",
    "        print(\"  - Regularization: MSE\")\n",
    "        \n",
    "        return loss_functions\n",
    "    \n",
    "    def create_simulated_batch(self, batch_size=4):\n",
    "        \"\"\"Create simulated training batch.\"\"\"\n",
    "        H, W = self.config['model']['image_size']\n",
    "        \n",
    "        # Simulate images\n",
    "        images = torch.randn(batch_size, 3, H, W).to(self.device)\n",
    "        \n",
    "        # Simulate detection targets\n",
    "        det_targets = []\n",
    "        for i in range(batch_size):\n",
    "            # Random number of objects\n",
    "            num_objects = torch.randint(1, 5, (1,)).item()\n",
    "            \n",
    "            # Random bboxes (normalized)\n",
    "            bboxes = torch.rand(num_objects, 4)\n",
    "            bboxes[:, 2:] = bboxes[:, 2:] * 0.3 + 0.1  # Reasonable sizes\n",
    "            \n",
    "            # Random classes\n",
    "            classes = torch.randint(0, self.config['model']['num_classes'], (num_objects,))\n",
    "            \n",
    "            det_targets.append({\n",
    "                'bboxes': bboxes,\n",
    "                'classes': classes\n",
    "            })\n",
    "        \n",
    "        # Simulate classification targets\n",
    "        cls_targets = torch.randint(0, self.config['model']['num_classes'], (batch_size,))\n",
    "        \n",
    "        return images, det_targets, cls_targets\n",
    "    \n",
    "    def training_step(self, batch_size=4):\n",
    "        \"\"\"Simulate a single training step.\"\"\"\n",
    "        self.model.train()\n",
    "        \n",
    "        # Create simulated batch\n",
    "        images, det_targets, cls_targets = self.create_simulated_batch(batch_size)\n",
    "        \n",
    "        # Mixed precision context\n",
    "        with autocast(enabled=self.config['training']['use_amp']):\n",
    "            # Forward pass\n",
    "            outputs = self.model(images, task='detection')\n",
    "            \n",
    "            # Calculate losses\n",
    "            det_loss = self.loss_functions['detection'](\n",
    "                outputs['detections'], det_targets\n",
    "            )\n",
    "            \n",
    "            # Optional classification loss\n",
    "            if 'classifications' in outputs:\n",
    "                cls_loss = self.loss_functions['classification'](\n",
    "                    outputs['classifications'], cls_targets.to(self.device)\n",
    "                )\n",
    "                total_loss = det_loss['total'] + 0.1 * cls_loss\n",
    "            else:\n",
    "                total_loss = det_loss['total']\n",
    "        \n",
    "        # Backward pass\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        if self.config['training']['use_amp']:\n",
    "            scaler = GradScaler()\n",
    "            scaler.scale(total_loss).backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            scaler.unscale_(self.optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                self.model.parameters(),\n",
    "                max_norm=self.config['training']['gradient_clip']\n",
    "            )\n",
    "            \n",
    "            # Optimizer step\n",
    "            scaler.step(self.optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            total_loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                self.model.parameters(),\n",
    "                max_norm=self.config['training']['gradient_clip']\n",
    "            )\n",
    "            \n",
    "            # Optimizer step\n",
    "            self.optimizer.step()\n",
    "        \n",
    "        # Collect metrics\n",
    "        step_metrics = {\n",
    "            'total_loss': total_loss.item(),\n",
    "            'detection_loss': det_loss['total'].item() if isinstance(det_loss, dict) else det_loss.item(),\n",
    "            'learning_rate': self.optimizer.param_groups[0]['lr']\n",
    "        }\n",
    "        \n",
    "        # Add component losses if available\n",
    "        if isinstance(det_loss, dict):\n",
    "            for key, value in det_loss.items():\n",
    "                if key != 'total':\n",
    "                    step_metrics[f'det_{key}'] = value.item()\n",
    "        \n",
    "        # Collect gradient statistics\n",
    "        grad_stats = self.collect_gradient_statistics()\n",
    "        step_metrics.update(grad_stats)\n",
    "        \n",
    "        # Collect stability metrics\n",
    "        stability_metrics = self.model.get_stability_metrics()\n",
    "        step_metrics.update(stability_metrics)\n",
    "        \n",
    "        return step_metrics\n",
    "    \n",
    "    def collect_gradient_statistics(self):\n",
    "        \"\"\"Collect gradient statistics from model.\"\"\"\n",
    "        grad_stats = {\n",
    "            'grad_norm_total': 0,\n",
    "            'grad_norm_mean': 0,\n",
    "            'grad_norm_max': 0,\n",
    "            'grad_norm_min': float('inf'),\n",
    "            'grad_mean': 0,\n",
    "            'grad_std': 0\n",
    "        }\n",
    "        \n",
    "        all_grad_norms = []\n",
    "        all_grad_values = []\n",
    "        \n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                grad_norm = param.grad.norm().item()\n",
    "                grad_values = param.grad.cpu().flatten().numpy()\n",
    "                \n",
    "                grad_stats['grad_norm_total'] += grad_norm\n",
    "                all_grad_norms.append(grad_norm)\n",
    "                all_grad_values.extend(grad_values.tolist())\n",
    "        \n",
    "        if all_grad_norms:\n",
    "            grad_stats['grad_norm_mean'] = np.mean(all_grad_norms)\n",
    "            grad_stats['grad_norm_max'] = np.max(all_grad_norms)\n",
    "            grad_stats['grad_norm_min'] = np.min(all_grad_norms)\n",
    "        \n",
    "        if all_grad_values:\n",
    "            grad_stats['grad_mean'] = np.mean(all_grad_values)\n",
    "            grad_stats['grad_std'] = np.std(all_grad_values)\n",
    "        \n",
    "        return grad_stats\n",
    "    \n",
    "    def simulate_training(self, num_steps=1000, log_interval=100):\n",
    "        \"\"\"Simulate training for analysis.\"\"\"\n",
    "        print(f\"\\nSimulating training for {num_steps} steps...\")\n",
    "        print(f\"Log interval: {log_interval} steps\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        for step in range(num_steps):\n",
    "            # Training step\n",
    "            step_metrics = self.training_step(batch_size=self.config['training']['batch_size'])\n",
    "            \n",
    "            # Update scheduler\n",
    "            if self.scheduler is not None:\n",
    "                self.scheduler.step()\n",
    "            \n",
    "            # Store in history\n",
    "            self.history['losses'].append(step_metrics['total_loss'])\n",
    "            self.history['learning_rates'].append(step_metrics['learning_rate'])\n",
    "            \n",
    "            # Store gradient stats\n",
    "            grad_stats = {k: v for k, v in step_metrics.items() if 'grad' in k}\n",
    "            self.history['gradients'].append(grad_stats)\n",
    "            \n",
    "            # Store stability metrics\n",
    "            stability_keys = ['max_eigenvalue', 'min_eigenvalue', 'signal_ratio_mean']\n",
    "            stability_metrics = {k: step_metrics.get(k, 0) for k in stability_keys}\n",
    "            self.history['stability_metrics'].append(stability_metrics)\n",
    "            \n",
    "            # Log progress\n",
    "            if (step + 1) % log_interval == 0:\n",
    "                print(f\"Step {step + 1}/{num_steps}:\")\n",
    "                print(f\"  Loss: {step_metrics['total_loss']:.4f}\")\n",
    "                print(f\"  LR: {step_metrics['learning_rate']:.6f}\")\n",
    "                print(f\"  Grad norm: {step_metrics.get('grad_norm_total', 0):.4f}\")\n",
    "                \n",
    "                if 'max_eigenvalue' in step_metrics:\n",
    "                    print(f\"  Max eigenvalue: {step_metrics['max_eigenvalue']:.4f}\")\n",
    "        \n",
    "        print(\"\\nTraining simulation completed!\")\n",
    "        \n",
    "        # Convert history to numpy arrays for easier analysis\n",
    "        self.process_history()\n",
    "        \n",
    "        return self.history\n",
    "    \n",
    "    def process_history(self):\n",
    "        \"\"\"Process training history for analysis.\"\"\"\n",
    "        # Convert lists to numpy arrays\n",
    "        self.history['losses_array'] = np.array(self.history['losses'])\n",
    "        self.history['learning_rates_array'] = np.array(self.history['learning_rates'])\n",
    "        \n",
    "        # Process gradient statistics\n",
    "        grad_keys = ['grad_norm_total', 'grad_norm_mean', 'grad_norm_max', \n",
    "                    'grad_norm_min', 'grad_mean', 'grad_std']\n",
    "        \n",
    "        for key in grad_keys:\n",
    "            values = [grad[key] for grad in self.history['gradients']]\n",
    "            self.history[f'{key}_array'] = np.array(values)\n",
    "        \n",
    "        # Process stability metrics\n",
    "        stability_keys = ['max_eigenvalue', 'min_eigenvalue', 'signal_ratio_mean']\n",
    "        for key in stability_keys:\n",
    "            values = [metrics.get(key, 0) for metrics in self.history['stability_metrics']]\n",
    "            self.history[f'{key}_array'] = np.array(values)\n",
    "\n",
    "# %%\n",
    "# Create training simulator\n",
    "simulator = TrainingSimulator(config)\n",
    "\n",
    "# Simulate training\n",
    "history = simulator.simulate_training(num_steps=500, log_interval=50)\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 3. Loss Function Analysis\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "class LossAnalyzer:\n",
    "    \"\"\"Analyze loss functions and convergence.\"\"\"\n",
    "    \n",
    "    def __init__(self, history):\n",
    "        self.history = history\n",
    "        \n",
    "    def analyze_loss_convergence(self):\n",
    "        \"\"\"Analyze loss convergence patterns.\"\"\"\n",
    "        print(\"\\nAnalyzing Loss Convergence:\")\n",
    "        \n",
    "        losses = self.history['losses_array']\n",
    "        \n",
    "        if len(losses) == 0:\n",
    "            print(\"No loss data available.\")\n",
    "            return\n",
    "        \n",
    "        # Calculate convergence metrics\n",
    "        initial_loss = losses[0] if len(losses) > 0 else 0\n",
    "        final_loss = losses[-1] if len(losses) > 0 else 0\n",
    "        min_loss = np.min(losses) if len(losses) > 0 else 0\n",
    "        \n",
    "        loss_reduction = ((initial_loss - final_loss) / initial_loss * 100) if initial_loss > 0 else 0\n",
    "        \n",
    "        print(f\"  Initial loss: {initial_loss:.4f}\")\n",
    "        print(f\"  Final loss: {final_loss:.4f}\")\n",
    "        print(f\"  Minimum loss: {min_loss:.4f}\")\n",
    "        print(f\"  Loss reduction: {loss_reduction:.1f}%\")\n",
    "        \n",
    "        # Analyze convergence rate\n",
    "        window_size = min(50, len(losses) // 10)\n",
    "        if window_size > 1:\n",
    "            smoothed = np.convolve(losses, np.ones(window_size)/window_size, mode='valid')\n",
    "            \n",
    "            if len(smoothed) > 1:\n",
    "                convergence_rate = abs(smoothed[-1] - smoothed[0]) / len(smoothed)\n",
    "                print(f\"  Convergence rate: {convergence_rate:.6f} per step\")\n",
    "        \n",
    "        # Check for convergence issues\n",
    "        self.detect_convergence_issues(losses)\n",
    "        \n",
    "        # Visualize loss convergence\n",
    "        self.visualize_loss_convergence(losses)\n",
    "    \n",
    "    def detect_convergence_issues(self, losses):\n",
    "        \"\"\"Detect potential convergence issues.\"\"\"\n",
    "        print(\"\\nConvergence Health Check:\")\n",
    "        \n",
    "        # Check for NaN or Inf\n",
    "        if np.any(np.isnan(losses)) or np.any(np.isinf(losses)):\n",
    "            print(\"  ❌ NaN or Inf detected in losses\")\n",
    "        else:\n",
    "            print(\"  ✅ No NaN/Inf values\")\n",
    "        \n",
    "        # Check for explosion\n",
    "        if np.max(losses) > 1000:\n",
    "            print(\"  ⚠️ Loss explosion detected\")\n",
    "        else:\n",
    "            print(\"  ✅ Loss values stable\")\n",
    "        \n",
    "        # Check for oscillation\n",
    "        if len(losses) > 10:\n",
    "            diff = np.diff(losses)\n",
    "            oscillation_score = np.std(diff) / (np.mean(np.abs(diff)) + 1e-8)\n",
    "            \n",
    "            if oscillation_score > 2.0:\n",
    "                print(f\"  ⚠️ High oscillation detected (score: {oscillation_score:.2f})\")\n",
    "            else:\n",
    "                print(f\"  ✅ Stable convergence (oscillation score: {oscillation_score:.2f})\")\n",
    "        \n",
    "        # Check for plateau\n",
    "        if len(losses) > 100:\n",
    "            last_100 = losses[-100:]\n",
    "            plateau_score = np.std(last_100) / (np.mean(last_100) + 1e-8)\n",
    "            \n",
    "            if plateau_score < 0.01:\n",
    "                print(f\"  ⚠️ Possible plateau (variance: {plateau_score:.4f})\")\n",
    "            else:\n",
    "                print(f\"  ✅ Active learning (variance: {plateau_score:.4f})\")\n",
    "    \n",
    "    def visualize_loss_convergence(self, losses):\n",
    "        \"\"\"Visualize loss convergence.\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "        \n",
    "        # Raw loss curve\n",
    "        axes[0, 0].plot(losses, linewidth=2, alpha=0.7)\n",
    "        axes[0, 0].set_xlabel('Training Step')\n",
    "        axes[0, 0].set_ylabel('Loss')\n",
    "        axes[0, 0].set_title('Training Loss Curve')\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add moving average\n",
    "        window_size = min(50, len(losses) // 10)\n",
    "        if window_size > 1:\n",
    "            moving_avg = np.convolve(losses, np.ones(window_size)/window_size, mode='valid')\n",
    "            x_avg = np.arange(window_size-1, len(losses))\n",
    "            axes[0, 0].plot(x_avg, moving_avg, 'r--', linewidth=2, label=f'MA({window_size})')\n",
    "            axes[0, 0].legend()\n",
    "        \n",
    "        # Log scale loss\n",
    "        axes[0, 1].semilogy(losses, linewidth=2, alpha=0.7)\n",
    "        axes[0, 1].set_xlabel('Training Step')\n",
    "        axes[0, 1].set_ylabel('Loss (log scale)')\n",
    "        axes[0, 1].set_title('Log-Scale Loss Curve')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Loss distribution\n",
    "        axes[1, 0].hist(losses, bins=50, alpha=0.7, edgecolor='black')\n",
    "        axes[1, 0].axvline(np.mean(losses), color='red', linestyle='--', \n",
    "                          label=f'Mean: {np.mean(losses):.4f}')\n",
    "        axes[1, 0].axvline(np.median(losses), color='green', linestyle='--',\n",
    "                          label=f'Median: {np.median(losses):.4f}')\n",
    "        axes[1, 0].set_xlabel('Loss Value')\n",
    "        axes[1, 0].set_ylabel('Frequency')\n",
    "        axes[1, 0].set_title('Loss Value Distribution')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Loss difference (gradient of loss)\n",
    "        if len(losses) > 1:\n",
    "            loss_diff = np.diff(losses)\n",
    "            axes[1, 1].plot(loss_diff, alpha=0.7)\n",
    "            axes[1, 1].axhline(y=0, color='r', linestyle='-', alpha=0.3)\n",
    "            axes[1, 1].set_xlabel('Training Step')\n",
    "            axes[1, 1].set_ylabel('Δ Loss')\n",
    "            axes[1, 1].set_title('Loss Changes (Gradient)')\n",
    "            axes[1, 1].grid(True, alpha=0.3)\n",
    "            \n",
    "            # Add statistics\n",
    "            pos_changes = np.sum(loss_diff > 0)\n",
    "            neg_changes = np.sum(loss_diff < 0)\n",
    "            axes[1, 1].text(0.05, 0.95, \n",
    "                           f'↑ Increases: {pos_changes}\\n↓ Decreases: {neg_changes}',\n",
    "                           transform=axes[1, 1].transAxes,\n",
    "                           verticalalignment='top',\n",
    "                           bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Create interactive visualization\n",
    "        self.create_interactive_loss_viz(losses)\n",
    "    \n",
    "    def create_interactive_loss_viz(self, losses):\n",
    "        \"\"\"Create interactive loss visualization.\"\"\"\n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=2,\n",
    "            subplot_titles=('Training Loss Curve', 'Log-Scale Loss',\n",
    "                           'Loss Distribution', 'Loss Changes'),\n",
    "            vertical_spacing=0.1\n",
    "        )\n",
    "        \n",
    "        # Loss curve\n",
    "        fig.add_trace(\n",
    "            go.Scatter(y=losses, mode='lines', name='Loss',\n",
    "                      line=dict(width=2, color='blue')),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # Add moving average\n",
    "        window_size = min(50, len(losses) // 10)\n",
    "        if window_size > 1:\n",
    "            moving_avg = np.convolve(losses, np.ones(window_size)/window_size, mode='valid')\n",
    "            x_avg = np.arange(window_size-1, len(losses))\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=x_avg, y=moving_avg, mode='lines',\n",
    "                          name=f'MA({window_size})', line=dict(dash='dash', color='red')),\n",
    "                row=1, col=1\n",
    "            )\n",
    "        \n",
    "        # Log scale loss\n",
    "        fig.add_trace(\n",
    "            go.Scatter(y=losses, mode='lines', name='Loss (log)',\n",
    "                      line=dict(width=2, color='green')),\n",
    "            row=1, col=2\n",
    "        )\n",
    "        \n",
    "        # Update y-axis to log scale\n",
    "        fig.update_yaxes(type=\"log\", row=1, col=2)\n",
    "        \n",
    "        # Loss distribution\n",
    "        fig.add_trace(\n",
    "            go.Histogram(x=losses, nbinsx=50, name='Loss Distribution',\n",
    "                        marker_color='purple'),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        \n",
    "        # Add mean and median lines\n",
    "        fig.add_vline(x=np.mean(losses), line_dash=\"dash\", line_color=\"red\",\n",
    "                     annotation_text=f\"Mean: {np.mean(losses):.4f}\", row=2, col=1)\n",
    "        fig.add_vline(x=np.median(losses), line_dash=\"dash\", line_color=\"green\",\n",
    "                     annotation_text=f\"Median: {np.median(losses):.4f}\", row=2, col=1)\n",
    "        \n",
    "        # Loss changes\n",
    "        if len(losses) > 1:\n",
    "            loss_diff = np.diff(losses)\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Scatter(y=loss_diff, mode='lines', name='Δ Loss',\n",
    "                          line=dict(width=1, color='orange')),\n",
    "                row=2, col=2\n",
    "            )\n",
    "            \n",
    "            # Add zero line\n",
    "            fig.add_hline(y=0, line_dash=\"dash\", line_color=\"red\", row=2, col=2)\n",
    "        \n",
    "        fig.update_layout(\n",
    "            height=800,\n",
    "            width=1200,\n",
    "            title_text=\"Loss Convergence Analysis\",\n",
    "            showlegend=True\n",
    "        )\n",
    "        \n",
    "        fig.show()\n",
    "    \n",
    "    def analyze_loss_components(self, history):\n",
    "        \"\"\"Analyze individual loss components.\"\"\"\n",
    "        print(\"\\nAnalyzing Loss Components:\")\n",
    "        \n",
    "        # Extract component losses from history\n",
    "        component_keys = [key for key in history.keys() if 'det_' in key]\n",
    "        \n",
    "        if not component_keys:\n",
    "            print(\"  No component loss data available.\")\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(1, len(component_keys), figsize=(4*len(component_keys), 5))\n",
    "        \n",
    "        if len(component_keys) == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for idx, key in enumerate(component_keys):\n",
    "            # Extract component values\n",
    "            values = []\n",
    "            for metrics in self.history['stability_metrics']:\n",
    "                if key in metrics:\n",
    "                    values.append(metrics[key])\n",
    "                elif f'{key}_array' in self.history:\n",
    "                    values = self.history[f'{key}_array']\n",
    "                    break\n",
    "            \n",
    "            if values:\n",
    "                values = np.array(values[:len(self.history['losses'])])\n",
    "                \n",
    "                axes[idx].plot(values, alpha=0.7)\n",
    "                axes[idx].set_xlabel('Training Step')\n",
    "                axes[idx].set_ylabel('Loss Value')\n",
    "                axes[idx].set_title(f'{key.replace(\"det_\", \"\").title()} Loss')\n",
    "                axes[idx].grid(True, alpha=0.3)\n",
    "                \n",
    "                # Add statistics\n",
    "                mean_val = np.mean(values)\n",
    "                axes[idx].axhline(y=mean_val, color='r', linestyle='--',\n",
    "                                 label=f'Mean: {mean_val:.4f}')\n",
    "                axes[idx].legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Analyze component contributions\n",
    "        self.analyze_component_contributions(component_keys)\n",
    "\n",
    "# %%\n",
    "# Analyze loss convergence\n",
    "loss_analyzer = LossAnalyzer(history)\n",
    "loss_analyzer.analyze_loss_convergence()\n",
    "\n",
    "# Analyze loss components\n",
    "loss_analyzer.analyze_loss_components(history)\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 4. Gradient Flow Analysis\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "class GradientAnalyzer:\n",
    "    \"\"\"Analyze gradient flow during training.\"\"\"\n",
    "    \n",
    "    def __init__(self, history):\n",
    "        self.history = history\n",
    "        \n",
    "    def analyze_gradient_flow(self):\n",
    "        \"\"\"Analyze gradient flow statistics.\"\"\"\n",
    "        print(\"\\nAnalyzing Gradient Flow:\")\n",
    "        \n",
    "        # Check if gradient data exists\n",
    "        if 'grad_norm_total_array' not in self.history:\n",
    "            print(\"  No gradient data available.\")\n",
    "            return\n",
    "        \n",
    "        grad_norms = self.history['grad_norm_total_array']\n",
    "        \n",
    "        if len(grad_norms) == 0:\n",
    "            print(\"  Empty gradient data.\")\n",
    "            return\n",
    "        \n",
    "        # Calculate statistics\n",
    "        mean_grad = np.mean(grad_norms)\n",
    "        std_grad = np.std(grad_norms)\n",
    "        max_grad = np.max(grad_norms)\n",
    "        min_grad = np.min(grad_norms)\n",
    "        \n",
    "        print(f\"  Mean gradient norm: {mean_grad:.4f}\")\n",
    "        print(f\"  Std gradient norm: {std_grad:.4f}\")\n",
    "        print(f\"  Max gradient norm: {max_grad:.4f}\")\n",
    "        print(f\"  Min gradient norm: {min_grad:.4f}\")\n",
    "        \n",
    "        # Analyze gradient health\n",
    "        self.analyze_gradient_health(grad_norms)\n",
    "        \n",
    "        # Visualize gradient flow\n",
    "        self.visualize_gradient_flow(grad_norms)\n",
    "        \n",
    "        # Analyze gradient distribution\n",
    "        if 'grad_mean_array' in self.history and 'grad_std_array' in self.history:\n",
    "            self.analyze_gradient_distribution()\n",
    "    \n",
    "    def analyze_gradient_health(self, grad_norms):\n",
    "        \"\"\"Analyze gradient health indicators.\"\"\"\n",
    "        print(\"\\nGradient Health Check:\")\n",
    "        \n",
    "        # Check for gradient explosion\n",
    "        explosion_threshold = 1000\n",
    "        if np.any(grad_norms > explosion_threshold):\n",
    "            explosion_count = np.sum(grad_norms > explosion_threshold)\n",
    "            print(f\"  ❌ Gradient explosion detected: {explosion_count} steps > {explosion_threshold}\")\n",
    "        else:\n",
    "            print(f\"  ✅ No gradient explosion (all < {explosion_threshold})\")\n",
    "        \n",
    "        # Check for gradient vanishing\n",
    "        vanishing_threshold = 1e-6\n",
    "        if np.any(grad_norms < vanishing_threshold):\n",
    "            vanishing_count = np.sum(grad_norms < vanishing_threshold)\n",
    "            print(f\"  ❌ Gradient vanishing detected: {vanishing_count} steps < {vanishing_threshold}\")\n",
    "        else:\n",
    "            print(f\"  ✅ No gradient vanishing (all > {vanishing_threshold})\")\n",
    "        \n",
    "        # Check gradient stability\n",
    "        grad_diff = np.diff(grad_norms)\n",
    "        grad_instability = np.std(grad_diff) / (np.mean(np.abs(grad_diff)) + 1e-8)\n",
    "        \n",
    "        if grad_instability > 5.0:\n",
    "            print(f\"  ⚠️ Unstable gradients (instability score: {grad_instability:.2f})\")\n",
    "        elif grad_instability > 2.0:\n",
    "            print(f\"  ⚠️ Moderately unstable gradients (score: {grad_instability:.2f})\")\n",
    "        else:\n",
    "            print(f\"  ✅ Stable gradients (instability score: {grad_instability:.2f})\")\n",
    "        \n",
    "        # Check gradient mean (should be close to 0)\n",
    "        if 'grad_mean_array' in self.history:\n",
    "            grad_means = self.history['grad_mean_array']\n",
    "            mean_of_means = np.mean(np.abs(grad_means))\n",
    "            \n",
    "            if mean_of_means > 0.1:\n",
    "                print(f\"  ⚠️ Gradient mean偏高 (absolute mean: {mean_of_means:.4f})\")\n",
    "            else:\n",
    "                print(f\"  ✅ Gradient mean接近零 (absolute mean: {mean_of_means:.4f})\")\n",
    "    \n",
    "    def visualize_gradient_flow(self, grad_norms):\n",
    "        \"\"\"Visualize gradient flow over training.\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "        \n",
    "        # Gradient norms over time\n",
    "        axes[0, 0].plot(grad_norms, alpha=0.7)\n",
    "        axes[0, 0].set_xlabel('Training Step')\n",
    "        axes[0, 0].set_ylabel('Gradient Norm')\n",
    "        axes[0, 0].set_title('Gradient Norms Over Time')\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add mean line\n",
    "        mean_grad = np.mean(grad_norms)\n",
    "        axes[0, 0].axhline(y=mean_grad, color='r', linestyle='--',\n",
    "                          label=f'Mean: {mean_grad:.4f}')\n",
    "        axes[0, 0].legend()\n",
    "        \n",
    "        # Log scale gradient norms\n",
    "        axes[0, 1].semilogy(grad_norms, alpha=0.7)\n",
    "        axes[0, 1].set_xlabel('Training Step')\n",
    "        axes[0, 1].set_ylabel('Gradient Norm (log scale)')\n",
    "        axes[0, 1].set_title('Log-Scale Gradient Norms')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Gradient norm distribution\n",
    "        axes[1, 0].hist(grad_norms, bins=50, alpha=0.7, edgecolor='black')\n",
    "        axes[1, 0].axvline(mean_grad, color='r', linestyle='--',\n",
    "                          label=f'Mean: {mean_grad:.4f}')\n",
    "        axes[1, 0].axvline(np.median(grad_norms), color='g', linestyle='--',\n",
    "                          label=f'Median: {np.median(grad_norms):.4f}')\n",
    "        axes[1, 0].set_xlabel('Gradient Norm')\n",
    "        axes[1, 0].set_ylabel('Frequency')\n",
    "        axes[1, 0].set_title('Gradient Norm Distribution')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Gradient changes\n",
    "        if len(grad_norms) > 1:\n",
    "            grad_changes = np.diff(grad_norms)\n",
    "            axes[1, 1].plot(grad_changes, alpha=0.7)\n",
    "            axes[1, 1].axhline(y=0, color='r', linestyle='-', alpha=0.3)\n",
    "            axes[1, 1].set_xlabel('Training Step')\n",
    "            axes[1, 1].set_ylabel('Δ Gradient Norm')\n",
    "            axes[1, 1].set_title('Gradient Norm Changes')\n",
    "            axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Create interactive visualization\n",
    "        self.create_interactive_gradient_viz(grad_norms)\n",
    "    \n",
    "    def create_interactive_gradient_viz(self, grad_norms):\n",
    "        \"\"\"Create interactive gradient visualization.\"\"\"\n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=2,\n",
    "            subplot_titles=('Gradient Norms', 'Log-Scale Gradient Norms',\n",
    "                           'Gradient Distribution', 'Gradient Changes'),\n",
    "            vertical_spacing=0.1\n",
    "        )\n",
    "        \n",
    "        # Gradient norms\n",
    "        fig.add_trace(\n",
    "            go.Scatter(y=grad_norms, mode='lines', name='Gradient Norm',\n",
    "                      line=dict(width=2, color='blue')),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # Add mean line\n",
    "        mean_grad = np.mean(grad_norms)\n",
    "        fig.add_hline(y=mean_grad, line_dash=\"dash\", line_color=\"red\",\n",
    "                     annotation_text=f\"Mean: {mean_grad:.4f}\", row=1, col=1)\n",
    "        \n",
    "        # Log scale\n",
    "        fig.add_trace(\n",
    "            go.Scatter(y=grad_norms, mode='lines', name='Gradient Norm (log)',\n",
    "                      line=dict(width=2, color='green')),\n",
    "            row=1, col=2\n",
    "        )\n",
    "        fig.update_yaxes(type=\"log\", row=1, col=2)\n",
    "        \n",
    "        # Distribution\n",
    "        fig.add_trace(\n",
    "            go.Histogram(x=grad_norms, nbinsx=50, name='Distribution',\n",
    "                        marker_color='purple'),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        \n",
    "        # Add mean and median\n",
    "        fig.add_vline(x=mean_grad, line_dash=\"dash\", line_color=\"red\",\n",
    "                     annotation_text=f\"Mean\", row=2, col=1)\n",
    "        fig.add_vline(x=np.median(grad_norms), line_dash=\"dash\", line_color=\"green\",\n",
    "                     annotation_text=f\"Median\", row=2, col=1)\n",
    "        \n",
    "        # Changes\n",
    "        if len(grad_norms) > 1:\n",
    "            grad_changes = np.diff(grad_norms)\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Scatter(y=grad_changes, mode='lines', name='Δ Gradient',\n",
    "                          line=dict(width=1, color='orange')),\n",
    "                row=2, col=2\n",
    "            )\n",
    "            \n",
    "            fig.add_hline(y=0, line_dash=\"dash\", line_color=\"red\", row=2, col=2)\n",
    "        \n",
    "        fig.update_layout(\n",
    "            height=800,\n",
    "            width=1200,\n",
    "            title_text=\"Gradient Flow Analysis\",\n",
    "            showlegend=True\n",
    "        )\n",
    "        \n",
    "        fig.show()\n",
    "    \n",
    "    def analyze_gradient_distribution(self):\n",
    "        \"\"\"Analyze gradient value distribution.\"\"\"\n",
    "        print(\"\\nAnalyzing Gradient Distribution:\")\n",
    "        \n",
    "        if 'grad_mean_array' not in self.history or 'grad_std_array' not in self.history:\n",
    "            print(\"  No gradient distribution data available.\")\n",
    "            return\n",
    "        \n",
    "        grad_means = self.history['grad_mean_array']\n",
    "        grad_stds = self.history['grad_std_array']\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        \n",
    "        # Gradient means over time\n",
    "        axes[0].plot(grad_means, alpha=0.7)\n",
    "        axes[0].axhline(y=0, color='r', linestyle='--', alpha=0.5)\n",
    "        axes[0].set_xlabel('Training Step')\n",
    "        axes[0].set_ylabel('Mean Gradient Value')\n",
    "        axes[0].set_title('Gradient Means Over Time')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Gradient standard deviations\n",
    "        axes[1].plot(grad_stds, alpha=0.7)\n",
    "        axes[1].set_xlabel('Training Step')\n",
    "        axes[1].set_ylabel('Gradient Std Dev')\n",
    "        axes[1].set_title('Gradient Standard Deviations')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print statistics\n",
    "        print(f\"  Mean of gradient means: {np.mean(grad_means):.6f}\")\n",
    "        print(f\"  Std of gradient means: {np.std(grad_means):.6f}\")\n",
    "        print(f\"  Mean gradient std: {np.mean(grad_stds):.6f}\")\n",
    "        \n",
    "        # Check for bias\n",
    "        mean_bias = np.mean(np.abs(grad_means))\n",
    "        if mean_bias > 0.01:\n",
    "            print(f\"  ⚠️ Significant gradient bias detected: {mean_bias:.6f}\")\n",
    "        else:\n",
    "            print(f\"  ✅ Minimal gradient bias: {mean_bias:.6f}\")\n",
    "\n",
    "# %%\n",
    "# Analyze gradient flow\n",
    "gradient_analyzer = GradientAnalyzer(history)\n",
    "gradient_analyzer.analyze_gradient_flow()\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 5. Learning Rate Analysis\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "class LearningRateAnalyzer:\n",
    "    \"\"\"Analyze learning rate scheduling and optimization.\"\"\"\n",
    "    \n",
    "    def __init__(self, history):\n",
    "        self.history = history\n",
    "        \n",
    "    def analyze_learning_rate(self):\n",
    "        \"\"\"Analyze learning rate schedule and effects.\"\"\"\n",
    "        print(\"\\nAnalyzing Learning Rate Schedule:\")\n",
    "        \n",
    "        if 'learning_rates_array' not in self.history:\n",
    "            print(\"  No learning rate data available.\")\n",
    "            return\n",
    "        \n",
    "        lrs = self.history['learning_rates_array']\n",
    "        \n",
    "        if len(lrs) == 0:\n",
    "            print(\"  Empty learning rate data.\")\n",
    "            return\n",
    "        \n",
    "        # Calculate statistics\n",
    "        initial_lr = lrs[0]\n",
    "        final_lr = lrs[-1]\n",
    "        min_lr = np.min(lrs)\n",
    "        max_lr = np.max(lrs)\n",
    "        \n",
    "        print(f\"  Initial LR: {initial_lr:.6f}\")\n",
    "        print(f\"  Final LR: {final_lr:.6f}\")\n",
    "        print(f\"  Minimum LR: {min_lr:.6f}\")\n",
    "        print(f\"  Maximum LR: {max_lr:.6f}\")\n",
    "        print(f\"  LR reduction: {(1 - final_lr/initial_lr)*100:.1f}%\")\n",
    "        \n",
    "        # Analyze schedule effectiveness\n",
    "        self.analyze_schedule_effectiveness(lrs)\n",
    "        \n",
    "        # Visualize learning rate schedule\n",
    "        self.visualize_learning_rate(lrs)\n",
    "        \n",
    "        # Analyze LR vs Loss correlation\n",
    "        if 'losses_array' in self.history:\n",
    "            self.analyze_lr_loss_correlation(lrs, self.history['losses_array'])\n",
    "    \n",
    "    def analyze_schedule_effectiveness(self, lrs):\n",
    "        \"\"\"Analyze learning rate schedule effectiveness.\"\"\"\n",
    "        print(\"\\nSchedule Effectiveness Analysis:\")\n",
    "        \n",
    "        # Calculate LR changes\n",
    "        lr_changes = np.diff(lrs)\n",
    "        \n",
    "        # Count increases and decreases\n",
    "        increases = np.sum(lr_changes > 0)\n",
    "        decreases = np.sum(lr_changes < 0)\n",
    "        constant = np.sum(lr_changes == 0)\n",
    "        \n",
    "        print(f\"  LR increases: {increases}\")\n",
    "        print(f\"  LR decreases: {decreases}\")\n",
    "        print(f\"  Constant LR steps: {constant}\")\n",
    "        \n",
    "        # Check schedule pattern\n",
    "        if decreases > increases * 5:  # Mostly decreasing\n",
    "            print(\"  ✅ Schedule: Mostly decreasing (good for convergence)\")\n",
    "        elif increases > decreases * 5:  # Mostly increasing\n",
    "            print(\"  ⚠️ Schedule: Mostly increasing (unusual)\")\n",
    "        else:  # Mixed\n",
    "            print(\"  ⚠️ Schedule: Mixed increases/decreases (cyclic schedule)\")\n",
    "        \n",
    "        # Check for proper decay\n",
    "        if lrs[-1] < lrs[0] * 0.1:  # Decayed by at least 90%\n",
    "            print(\"  ✅ Strong decay achieved\")\n",
    "        elif lrs[-1] < lrs[0] * 0.5:  # Decayed by at least 50%\n",
    "            print(\"  ✅ Moderate decay achieved\")\n",
    "        else:\n",
    "            print(\"  ⚠️ Limited decay achieved\")\n",
    "    \n",
    "    def visualize_learning_rate(self, lrs):\n",
    "        \"\"\"Visualize learning rate schedule.\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "        \n",
    "        # LR over time\n",
    "        axes[0, 0].plot(lrs, linewidth=2, color='darkblue')\n",
    "        axes[0, 0].set_xlabel('Training Step')\n",
    "        axes[0, 0].set_ylabel('Learning Rate')\n",
    "        axes[0, 0].set_title('Learning Rate Schedule')\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Log scale LR\n",
    "        axes[0, 1].semilogy(lrs, linewidth=2, color='darkgreen')\n",
    "        axes[0, 1].set_xlabel('Training Step')\n",
    "        axes[0, 1].set_ylabel('Learning Rate (log scale)')\n",
    "        axes[0, 1].set_title('Log-Scale Learning Rate')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # LR distribution\n",
    "        axes[1, 0].hist(lrs, bins=30, alpha=0.7, edgecolor='black', color='purple')\n",
    "        axes[1, 0].axvline(np.mean(lrs), color='red', linestyle='--',\n",
    "                          label=f'Mean: {np.mean(lrs):.6f}')\n",
    "        axes[1, 0].axvline(np.median(lrs), color='green', linestyle='--',\n",
    "                          label=f'Median: {np.median(lrs):.6f}')\n",
    "        axes[1, 0].set_xlabel('Learning Rate')\n",
    "        axes[1, 0].set_ylabel('Frequency')\n",
    "        axes[1, 0].set_title('Learning Rate Distribution')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # LR changes\n",
    "        if len(lrs) > 1:\n",
    "            lr_changes = np.diff(lrs)\n",
    "            axes[1, 1].plot(lr_changes, alpha=0.7, color='orange')\n",
    "            axes[1, 1].axhline(y=0, color='r', linestyle='-', alpha=0.3)\n",
    "            axes[1, 1].set_xlabel('Training Step')\n",
    "            axes[1, 1].set_ylabel('Δ Learning Rate')\n",
    "            axes[1, 1].set_title('Learning Rate Changes')\n",
    "            axes[1, 1].grid(True, alpha=0.3)\n",
    "            \n",
    "            # Add statistics\n",
    "            pos_changes = np.sum(lr_changes > 0)\n",
    "            neg_changes = np.sum(lr_changes < 0)\n",
    "            axes[1, 1].text(0.05, 0.95, \n",
    "                           f'↑ Increases: {pos_changes}\\n↓ Decreases: {neg_changes}',\n",
    "                           transform=axes[1, 1].transAxes,\n",
    "                           verticalalignment='top',\n",
    "                           bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Create interactive visualization\n",
    "        self.create_interactive_lr_viz(lrs)\n",
    "    \n",
    "    def create_interactive_lr_viz(self, lrs):\n",
    "        \"\"\"Create interactive learning rate visualization.\"\"\"\n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=2,\n",
    "            subplot_titles=('Learning Rate Schedule', 'Log-Scale Learning Rate',\n",
    "                           'Learning Rate Distribution', 'Learning Rate Changes'),\n",
    "            vertical_spacing=0.1\n",
    "        )\n",
    "        \n",
    "        # LR schedule\n",
    "        fig.add_trace(\n",
    "            go.Scatter(y=lrs, mode='lines', name='Learning Rate',\n",
    "                      line=dict(width=3, color='darkblue')),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # Log scale\n",
    "        fig.add_trace(\n",
    "            go.Scatter(y=lrs, mode='lines', name='LR (log)',\n",
    "                      line=dict(width=3, color='darkgreen')),\n",
    "            row=1, col=2\n",
    "        )\n",
    "        fig.update_yaxes(type=\"log\", row=1, col=2)\n",
    "        \n",
    "        # Distribution\n",
    "        fig.add_trace(\n",
    "            go.Histogram(x=lrs, nbinsx=30, name='Distribution',\n",
    "                        marker_color='purple'),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        \n",
    "        # Add mean and median\n",
    "        fig.add_vline(x=np.mean(lrs), line_dash=\"dash\", line_color=\"red\",\n",
    "                     annotation_text=f\"Mean: {np.mean(lrs):.6f}\", row=2, col=1)\n",
    "        fig.add_vline(x=np.median(lrs), line_dash=\"dash\", line_color=\"green\",\n",
    "                     annotation_text=f\"Median: {np.median(lrs):.6f}\", row=2, col=1)\n",
    "        \n",
    "        # Changes\n",
    "        if len(lrs) > 1:\n",
    "            lr_changes = np.diff(lrs)\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Scatter(y=lr_changes, mode='lines', name='Δ LR',\n",
    "                          line=dict(width=2, color='orange')),\n",
    "                row=2, col=2\n",
    "            )\n",
    "            \n",
    "            fig.add_hline(y=0, line_dash=\"dash\", line_color=\"red\", row=2, col=2)\n",
    "            \n",
    "            # Add statistics annotation\n",
    "            pos_changes = np.sum(lr_changes > 0)\n",
    "            neg_changes = np.sum(lr_changes < 0)\n",
    "            \n",
    "            fig.add_annotation(\n",
    "                x=0.05, y=0.95,\n",
    "                xref=\"paper\", yref=\"paper\",\n",
    "                text=f\"Increases: {pos_changes}<br>Decreases: {neg_changes}\",\n",
    "                showarrow=False,\n",
    "                font=dict(size=12),\n",
    "                align=\"left\",\n",
    "                bgcolor=\"white\",\n",
    "                bordercolor=\"black\",\n",
    "                borderwidth=1,\n",
    "                row=2, col=2\n",
    "            )\n",
    "        \n",
    "        fig.update_layout(\n",
    "            height=800,\n",
    "            width=1200,\n",
    "            title_text=\"Learning Rate Analysis\",\n",
    "            showlegend=True\n",
    "        )\n",
    "        \n",
    "        fig.show()\n",
    "    \n",
    "    def analyze_lr_loss_correlation(self, lrs, losses):\n",
    "        \"\"\"Analyze correlation between learning rate and loss.\"\"\"\n",
    "        print(\"\\nAnalyzing LR-Loss Correlation:\")\n",
    "        \n",
    "        if len(lrs) != len(losses):\n",
    "            print(f\"  Mismatched data lengths: LR={len(lrs)}, Loss={len(losses)}\")\n",
    "            # Align lengths\n",
    "            min_len = min(len(lrs), len(losses))\n",
    "            lrs = lrs[:min_len]\n",
    "            losses = losses[:min_len]\n",
    "        \n",
    "        # Calculate correlation\n",
    "        correlation = np.corrcoef(lrs, losses)[0, 1]\n",
    "        \n",
    "        print(f\"  Correlation coefficient: {correlation:.4f}\")\n",
    "        \n",
    "        if correlation > 0.3:\n",
    "            print(\"  ⚠️ Positive correlation: Higher LR → Higher loss\")\n",
    "        elif correlation < -0.3:\n",
    "            print(\"  ✅ Negative correlation: Higher LR → Lower loss (good)\")\n",
    "        else:\n",
    "            print(\"  ⚠️ Weak correlation: LR changes not strongly affecting loss\")\n",
    "        \n",
    "        # Create scatter plot\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        \n",
    "        # Scatter plot\n",
    "        scatter = axes[0].scatter(lrs, losses, alpha=0.6, c=range(len(lrs)), cmap='viridis')\n",
    "        axes[0].set_xlabel('Learning Rate')\n",
    "        axes[0].set_ylabel('Loss')\n",
    "        axes[0].set_title(f'LR vs Loss (Correlation: {correlation:.3f})')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        plt.colorbar(scatter, ax=axes[0], label='Training Step')\n",
    "        \n",
    "        # Rolling correlation\n",
    "        window_size = min(50, len(lrs) // 4)\n",
    "        if window_size > 1:\n",
    "            rolling_corr = []\n",
    "            for i in range(len(lrs) - window_size + 1):\n",
    "                lr_window = lrs[i:i+window_size]\n",
    "                loss_window = losses[i:i+window_size]\n",
    "                corr = np.corrcoef(lr_window, loss_window)[0, 1]\n",
    "                rolling_corr.append(corr)\n",
    "            \n",
    "            axes[1].plot(range(window_size-1, len(lrs)), rolling_corr)\n",
    "            axes[1].axhline(y=0, color='r', linestyle='--', alpha=0.5)\n",
    "            axes[1].set_xlabel('Training Step')\n",
    "            axes[1].set_ylabel('Correlation')\n",
    "            axes[1].set_title(f'Rolling Correlation (window={window_size})')\n",
    "            axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# %%\n",
    "# Analyze learning rate\n",
    "lr_analyzer = LearningRateAnalyzer(history)\n",
    "lr_analyzer.analyze_learning_rate()\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 6. Stability Analysis\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "class StabilityAnalyzer:\n",
    "    \"\"\"Analyze training stability metrics.\"\"\"\n",
    "    \n",
    "    def __init__(self, history):\n",
    "        self.history = history\n",
    "        \n",
    "    def analyze_stability(self):\n",
    "        \"\"\"Analyze training stability.\"\"\"\n",
    "        print(\"\\nAnalyzing Training Stability:\")\n",
    "        \n",
    "        # Check for key stability metrics\n",
    "        stability_metrics = ['max_eigenvalue_array', 'min_eigenvalue_array', 'signal_ratio_mean_array']\n",
    "        available_metrics = [m for m in stability_metrics if m in self.history]\n",
    "        \n",
    "        if not available_metrics:\n",
    "            print(\"  No stability metrics available.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"  Available stability metrics: {', '.join(available_metrics)}\")\n",
    "        \n",
    "        # Analyze each metric\n",
    "        for metric_name in available_metrics:\n",
    "            self.analyze_stability_metric(metric_name)\n",
    "        \n",
    "        # Analyze overall stability\n",
    "        self.analyze_overall_stability(available_metrics)\n",
    "    \n",
    "    def analyze_stability_metric(self, metric_name):\n",
    "        \"\"\"Analyze a specific stability metric.\"\"\"\n",
    "        print(f\"\\nAnalyzing {metric_name}:\")\n",
    "        \n",
    "        values = self.history[metric_name]\n",
    "        \n",
    "        if len(values) == 0:\n",
    "            print(\"  Empty data.\")\n",
    "            return\n",
    "        \n",
    "        # Calculate statistics\n",
    "        mean_val = np.mean(values)\n",
    "        std_val = np.std(values)\n",
    "        min_val = np.min(values)\n",
    "        max_val = np.max(values)\n",
    "        \n",
    "        print(f\"  Mean: {mean_val:.6f}\")\n",
    "        print(f\"  Std: {std_val:.6f}\")\n",
    "        print(f\"  Range: [{min_val:.6f}, {max_val:.6f}]\")\n",
    "        \n",
    "        # Metric-specific analysis\n",
    "        if 'eigenvalue' in metric_name:\n",
    "            if 'max' in metric_name:\n",
    "                if max_val > 1.0:\n",
    "                    print(f\"  ❌ Max eigenvalue > 1.0: {max_val:.6f}\")\n",
    "                elif max_val > 0.95:\n",
    "                    print(f\"  ⚠️ Max eigenvalue接近1.0: {max_val:.6f}\")\n",
    "                else:\n",
    "                    print(f\"  ✅ Max eigenvalue < 1.0: {max_val:.6f}\")\n",
    "            \n",
    "            if 'min' in metric_name:\n",
    "                if min_val < 0:\n",
    "                    print(f\"  ⚠️ Min eigenvalue < 0: {min_val:.6f}\")\n",
    "                else:\n",
    "                    print(f\"  ✅ Min eigenvalue ≥ 0: {min_val:.6f}\")\n",
    "        \n",
    "        elif 'signal_ratio' in metric_name:\n",
    "            if abs(mean_val - 1.0) > 0.1:\n",
    "                print(f\"  ⚠️ Signal ratio偏离1.0: {mean_val:.4f}\")\n",
    "            else:\n",
    "                print(f\"  ✅ Signal ratio接近1.0: {mean_val:.4f}\")\n",
    "            \n",
    "            if std_val > 0.1:\n",
    "                print(f\"  ⚠️ High variance in signal ratio: {std_val:.4f}\")\n",
    "            else:\n",
    "                print(f\"  ✅ Stable signal ratio: {std_val:.4f}\")\n",
    "    \n",
    "    def analyze_overall_stability(self, available_metrics):\n",
    "        \"\"\"Analyze overall training stability.\"\"\"\n",
    "        print(\"\\nOverall Stability Assessment:\")\n",
    "        \n",
    "        stability_score = 0\n",
    "        max_score = len(available_metrics)\n",
    "        \n",
    "        # Check each metric\n",
    "        checks = []\n",
    "        \n",
    "        if 'max_eigenvalue_array' in available_metrics:\n",
    "            max_eigenvalues = self.history['max_eigenvalue_array']\n",
    "            if np.all(max_eigenvalues <= 1.0):\n",
    "                stability_score += 1\n",
    "                checks.append(\"✅ Max eigenvalue ≤ 1.0\")\n",
    "            else:\n",
    "                checks.append(\"❌ Max eigenvalue > 1.0\")\n",
    "        \n",
    "        if 'min_eigenvalue_array' in available_metrics:\n",
    "            min_eigenvalues = self.history['min_eigenvalue_array']\n",
    "            if np.all(min_eigenvalues >= 0):\n",
    "                stability_score += 1\n",
    "                checks.append(\"✅ Min eigenvalue ≥ 0\")\n",
    "            else:\n",
    "                checks.append(\"❌ Min eigenvalue < 0\")\n",
    "        \n",
    "        if 'signal_ratio_mean_array' in available_metrics:\n",
    "            signal_ratios = self.history['signal_ratio_mean_array']\n",
    "            if np.all(np.abs(signal_ratios - 1.0) < 0.2):\n",
    "                stability_score += 1\n",
    "                checks.append(\"✅ Signal ratio stable (~1.0)\")\n",
    "            else:\n",
    "                checks.append(\"❌ Signal ratio unstable\")\n",
    "        \n",
    "        # Calculate overall score\n",
    "        if max_score > 0:\n",
    "            overall_score = stability_score / max_score\n",
    "            print(f\"  Stability score: {stability_score}/{max_score} ({overall_score*100:.1f}%)\")\n",
    "            \n",
    "            if overall_score >= 0.8:\n",
    "                print(\"  🎉 EXCELLENT stability\")\n",
    "            elif overall_score >= 0.6:\n",
    "                print(\"  👍 GOOD stability\")\n",
    "            elif overall_score >= 0.4:\n",
    "                print(\"  ⚠️ MODERATE stability issues\")\n",
    "            else:\n",
    "                print(\"  ❌ POOR stability - needs attention\")\n",
    "        else:\n",
    "            print(\"  No stability metrics available for scoring.\")\n",
    "        \n",
    "        # Print individual checks\n",
    "        print(\"\\nStability Checks:\")\n",
    "        for check in checks:\n",
    "            print(f\"  {check}\")\n",
    "    \n",
    "    def visualize_stability_metrics(self):\n",
    "        \"\"\"Visualize stability metrics over training.\"\"\"\n",
    "        print(\"\\nVisualizing Stability Metrics...\")\n",
    "        \n",
    "        stability_metrics = ['max_eigenvalue_array', 'min_eigenvalue_array', 'signal_ratio_mean_array']\n",
    "        available_metrics = [m for m in stability_metrics if m in self.history]\n",
    "        \n",
    "        if not available_metrics:\n",
    "            print(\"  No stability metrics to visualize.\")\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(len(available_metrics), 2, figsize=(14, 4*len(available_metrics)))\n",
    "        \n",
    "        if len(available_metrics) == 1:\n",
    "            axes = axes.reshape(1, -1)\n",
    "        \n",
    "        for idx, metric_name in enumerate(available_metrics):\n",
    "            values = self.history[metric_name]\n",
    "            \n",
    "            # Time series\n",
    "            axes[idx, 0].plot(values, linewidth=2, alpha=0.7)\n",
    "            \n",
    "            # Add reference lines based on metric type\n",
    "            if 'eigenvalue' in metric_name:\n",
    "                if 'max' in metric_name:\n",
    "                    axes[idx, 0].axhline(y=1.0, color='r', linestyle='--', alpha=0.5, label='Stability bound')\n",
    "                elif 'min' in metric_name:\n",
    "                    axes[idx, 0].axhline(y=0.0, color='r', linestyle='--', alpha=0.5, label='Non-negativity')\n",
    "            \n",
    "            elif 'signal_ratio' in metric_name:\n",
    "                axes[idx, 0].axhline(y=1.0, color='r', linestyle='--', alpha=0.5, label='Ideal = 1.0')\n",
    "            \n",
    "            axes[idx, 0].set_xlabel('Training Step')\n",
    "            axes[idx, 0].set_ylabel(metric_name.replace('_array', ''))\n",
    "            axes[idx, 0].set_title(f'{metric_name.replace(\"_array\", \"\").replace(\"_\", \" \").title()}')\n",
    "            axes[idx, 0].legend()\n",
    "            axes[idx, 0].grid(True, alpha=0.3)\n",
    "            \n",
    "            # Distribution\n",
    "            axes[idx, 1].hist(values, bins=30, alpha=0.7, edgecolor='black')\n",
    "            axes[idx, 1].axvline(np.mean(values), color='red', linestyle='--',\n",
    "                                label=f'Mean: {np.mean(values):.4f}')\n",
    "            axes[idx, 1].axvline(np.median(values), color='green', linestyle='--',\n",
    "                                label=f'Median: {np.median(values):.4f}')\n",
    "            axes[idx, 1].set_xlabel('Value')\n",
    "            axes[idx, 1].set_ylabel('Frequency')\n",
    "            axes[idx, 1].set_title(f'Distribution of {metric_name.replace(\"_array\", \"\")}')\n",
    "            axes[idx, 1].legend()\n",
    "            axes[idx, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Create interactive visualization\n",
    "        self.create_interactive_stability_viz(available_metrics)\n",
    "\n",
    "# %%\n",
    "# Analyze stability\n",
    "stability_analyzer = StabilityAnalyzer(history)\n",
    "stability_analyzer.analyze_stability()\n",
    "stability_analyzer.visualize_stability_metrics()\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 7. Training Optimization Recommendations\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "class OptimizationRecommender:\n",
    "    \"\"\"Provide training optimization recommendations.\"\"\"\n",
    "    \n",
    "    def __init__(self, config, history, loss_analyzer, gradient_analyzer, \n",
    "                 lr_analyzer, stability_analyzer):\n",
    "        self.config = config\n",
    "        self.history = history\n",
    "        self.loss_analyzer = loss_analyzer\n",
    "        self.gradient_analyzer = gradient_analyzer\n",
    "        self.lr_analyzer = lr_analyzer\n",
    "        self.stability_analyzer = stability_analyzer\n",
    "        \n",
    "    def generate_recommendations(self):\n",
    "        \"\"\"Generate comprehensive optimization recommendations.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"TRAINING OPTIMIZATION RECOMMENDATIONS\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        recommendations = []\n",
    "        \n",
    "        # 1. Learning Rate Recommendations\n",
    "        lr_recs = self.analyze_learning_rate_optimization()\n",
    "        recommendations.extend(lr_recs)\n",
    "        \n",
    "        # 2. Gradient Flow Recommendations\n",
    "        grad_recs = self.analyze_gradient_optimization()\n",
    "        recommendations.extend(grad_recs)\n",
    "        \n",
    "        # 3. Stability Recommendations\n",
    "        stab_recs = self.analyze_stability_optimization()\n",
    "        recommendations.extend(stab_recs)\n",
    "        \n",
    "        # 4. General Training Recommendations\n",
    "        gen_recs = self.analyze_general_optimization()\n",
    "        recommendations.extend(gen_recs)\n",
    "        \n",
    "        # Display recommendations\n",
    "        self.display_recommendations(recommendations)\n",
    "        \n",
    "        # Generate summary\n",
    "        self.generate_summary(recommendations)\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def analyze_learning_rate_optimization(self):\n",
    "        \"\"\"Analyze and recommend LR optimizations.\"\"\"\n",
    "        recommendations = []\n",
    "        \n",
    "        if 'learning_rates_array' in self.history:\n",
    "            lrs = self.history['learning_rates_array']\n",
    "            initial_lr = lrs[0]\n",
    "            final_lr = lrs[-1]\n",
    "            \n",
    "            # Check initial LR\n",
    "            if initial_lr > 1e-2:\n",
    "                recommendations.append({\n",
    "                    'category': 'Learning Rate',\n",
    "                    'issue': 'High initial learning rate',\n",
    "                    'recommendation': 'Reduce initial LR to 1e-3 or 5e-4',\n",
    "                    'priority': 'HIGH',\n",
    "                    'rationale': f'Current LR {initial_lr:.1e} may cause instability'\n",
    "                })\n",
    "            elif initial_lr < 1e-4:\n",
    "                recommendations.append({\n",
    "                    'category': 'Learning Rate',\n",
    "                    'issue': 'Low initial learning rate',\n",
    "                    'recommendation': 'Increase initial LR to 1e-3',\n",
    "                    'priority': 'MEDIUM',\n",
    "                    'rationale': f'Current LR {initial_lr:.1e} may slow convergence'\n",
    "                })\n",
    "            \n",
    "            # Check LR decay\n",
    "            decay_ratio = final_lr / initial_lr\n",
    "            if decay_ratio > 0.5:\n",
    "                recommendations.append({\n",
    "                    'category': 'Learning Rate',\n",
    "                    'issue': 'Insufficient LR decay',\n",
    "                    'recommendation': 'Increase decay factor or add more decay steps',\n",
    "                    'priority': 'MEDIUM',\n",
    "                    'rationale': f'Only {100*(1-decay_ratio):.1f}% decay achieved'\n",
    "                })\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def analyze_gradient_optimization(self):\n",
    "        \"\"\"Analyze and recommend gradient optimizations.\"\"\"\n",
    "        recommendations = []\n",
    "        \n",
    "        if 'grad_norm_total_array' in self.history:\n",
    "            grad_norms = self.history['grad_norm_total_array']\n",
    "            mean_grad = np.mean(grad_norms)\n",
    "            max_grad = np.max(grad_norms)\n",
    "            \n",
    "            # Check for gradient explosion\n",
    "            if max_grad > 1000:\n",
    "                recommendations.append({\n",
    "                    'category': 'Gradient Flow',\n",
    "                    'issue': 'Gradient explosion detected',\n",
    "                    'recommendation': 'Reduce LR or increase gradient clipping',\n",
    "                    'priority': 'HIGH',\n",
    "                    'rationale': f'Max gradient norm: {max_grad:.1f}'\n",
    "                })\n",
    "            \n",
    "            # Check for gradient vanishing\n",
    "            if mean_grad < 1e-6:\n",
    "                recommendations.append({\n",
    "                    'category': 'Gradient Flow',\n",
    "                    'issue': 'Gradient vanishing detected',\n",
    "                    'recommendation': 'Use gradient clipping lower bound or skip connections',\n",
    "                    'priority': 'HIGH',\n",
    "                    'rationale': f'Mean gradient norm: {mean_grad:.1e}'\n",
    "                })\n",
    "            \n",
    "            # Check gradient clipping\n",
    "            current_clip = self.config['training']['gradient_clip']\n",
    "            if mean_grad > current_clip * 0.5:\n",
    "                recommendations.append({\n",
    "                    'category': 'Gradient Flow',\n",
    "                    'issue': 'Aggressive gradient clipping',\n",
    "                    'recommendation': f'Increase clip norm from {current_clip} to {current_clip*2}',\n",
    "                    'priority': 'MEDIUM',\n",
    "                    'rationale': f'Mean gradient {mean_grad:.2f}接近clip value {current_clip}'\n",
    "                })\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def analyze_stability_optimization(self):\n",
    "        \"\"\"Analyze and recommend stability optimizations.\"\"\"\n",
    "        recommendations = []\n",
    "        \n",
    "        # Check eigenvalue stability\n",
    "        if 'max_eigenvalue_array' in self.history:\n",
    "            max_eigenvalues = self.history['max_eigenvalue_array']\n",
    "            if np.any(max_eigenvalues > 1.0):\n",
    "                recommendations.append({\n",
    "                    'category': 'Stability',\n",
    "                    'issue': 'Eigenvalue > 1.0 detected',\n",
    "                    'recommendation': 'Increase Sinkhorn iterations or add eigenvalue penalty',\n",
    "                    'priority': 'HIGH',\n",
    "                    'rationale': 'Violates non-expansive mapping guarantee'\n",
    "                })\n",
    "        \n",
    "        # Check signal preservation\n",
    "        if 'signal_ratio_mean_array' in self.history:\n",
    "            signal_ratios = self.history['signal_ratio_mean_array']\n",
    "            mean_ratio = np.mean(signal_ratios)\n",
    "            \n",
    "            if abs(mean_ratio - 1.0) > 0.2:\n",
    "                recommendations.append({\n",
    "                    'category': 'Stability',\n",
    "                    'issue': 'Poor signal preservation',\n",
    "                    'recommendation': 'Adjust MHC initialization or add signal preservation loss',\n",
    "                    'priority': 'MEDIUM',\n",
    "                    'rationale': f'Signal ratio: {mean_ratio:.3f} (should be ~1.0)'\n",
    "                })\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def analyze_general_optimization(self):\n",
    "        \"\"\"Analyze and recommend general optimizations.\"\"\"\n",
    "        recommendations = []\n",
    "        \n",
    "        # Check batch size\n",
    "        batch_size = self.config['training']['batch_size']\n",
    "        if batch_size < 8:\n",
    "            recommendations.append({\n",
    "                'category': 'General',\n",
    "                'issue': 'Small batch size',\n",
    "                'recommendation': 'Increase batch size for better gradient estimates',\n",
    "                'priority': 'MEDIUM',\n",
    "                'rationale': f'Current batch size: {batch_size}'\n",
    "            })\n",
    "        elif batch_size > 32 and not self.config['training']['use_amp']:\n",
    "            recommendations.append({\n",
    "                'category': 'General',\n",
    "                'issue': 'Large batch size without mixed precision',\n",
    "                'recommendation': 'Enable mixed precision training',\n",
    "                'priority': 'HIGH',\n",
    "                'rationale': 'Will reduce memory and speed up training'\n",
    "            })\n",
    "        \n",
    "        # Check weight decay\n",
    "        weight_decay = self.config['training']['weight_decay']\n",
    "        if weight_decay < 1e-5:\n",
    "            recommendations.append({\n",
    "                'category': 'Regularization',\n",
    "                'issue': 'Weak weight decay',\n",
    "                'recommendation': 'Increase weight decay to 1e-4',\n",
    "                'priority': 'LOW',\n",
    "                'rationale': f'Current weight decay: {weight_decay:.1e}'\n",
    "            })\n",
    "        elif weight_decay > 1e-3:\n",
    "            recommendations.append({\n",
    "                'category': 'Regularization',\n",
    "                'issue': 'Strong weight decay',\n",
    "                'recommendation': 'Decrease weight decay to 1e-4',\n",
    "                'priority': 'MEDIUM',\n",
    "                'rationale': f'Current weight decay: {weight_decay:.1e}'\n",
    "            })\n",
    "        \n",
    "        # Check for mixed precision\n",
    "        if not self.config['training']['use_amp'] and self.config['device'] == 'cuda':\n",
    "            recommendations.append({\n",
    "                'category': 'Performance',\n",
    "                'issue': 'Mixed precision disabled',\n",
    "                'recommendation': 'Enable mixed precision training',\n",
    "                'priority': 'HIGH',\n",
    "                'rationale': 'Will significantly speed up training on GPU'\n",
    "            })\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def display_recommendations(self, recommendations):\n",
    "        \"\"\"Display recommendations in formatted table.\"\"\"\n",
    "        if not recommendations:\n",
    "            print(\"\\nNo optimization recommendations.\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\nDetailed Recommendations:\")\n",
    "        print(\"-\" * 120)\n",
    "        print(f\"{'Category':<15} {'Priority':<10} {'Issue':<40} {'Recommendation':<50}\")\n",
    "        print(\"-\" * 120)\n",
    "        \n",
    "        # Sort by priority\n",
    "        priority_order = {'HIGH': 0, 'MEDIUM': 1, 'LOW': 2}\n",
    "        recommendations.sort(key=lambda x: priority_order[x['priority']])\n",
    "        \n",
    "        for rec in recommendations:\n",
    "            # Color code priority\n",
    "            if rec['priority'] == 'HIGH':\n",
    "                priority_str = f\"\\033[91m{rec['priority']}\\033[0m\"\n",
    "            elif rec['priority'] == 'MEDIUM':\n",
    "                priority_str = f\"\\033[93m{rec['priority']}\\033[0m\"\n",
    "            else:\n",
    "                priority_str = f\"\\033[92m{rec['priority']}\\033[0m\"\n",
    "            \n",
    "            # Truncate long strings\n",
    "            issue = rec['issue'][:38] + '..' if len(rec['issue']) > 40 else rec['issue']\n",
    "            recommendation = rec['recommendation'][:48] + '..' if len(rec['recommendation']) > 50 else rec['recommendation']\n",
    "            \n",
    "            print(f\"{rec['category']:<15} {priority_str:<10} {issue:<40} {recommendation:<50}\")\n",
    "        \n",
    "        print(\"-\" * 120)\n",
    "        \n",
    "        # Count by priority\n",
    "        high_count = sum(1 for r in recommendations if r['priority'] == 'HIGH')\n",
    "        medium_count = sum(1 for r in recommendations if r['priority'] == 'MEDIUM')\n",
    "        low_count = sum(1 for r in recommendations if r['priority'] == 'LOW')\n",
    "        \n",
    "        print(f\"\\nPriority Summary: HIGH={high_count}, MEDIUM={medium_count}, LOW={low_count}\")\n",
    "    \n",
    "    def generate_summary(self, recommendations):\n",
    "        \"\"\"Generate optimization summary.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"OPTIMIZATION SUMMARY\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Count recommendations by category\n",
    "        categories = {}\n",
    "        for rec in recommendations:\n",
    "            cat = rec['category']\n",
    "            if cat not in categories:\n",
    "                categories[cat] = 0\n",
    "            categories[cat] += 1\n",
    "        \n",
    "        print(\"\\nRecommendations by Category:\")\n",
    "        for cat, count in categories.items():\n",
    "            print(f\"  {cat}: {count}\")\n",
    "        \n",
    "        # Top recommendations\n",
    "        high_recs = [r for r in recommendations if r['priority'] == 'HIGH']\n",
    "        if high_recs:\n",
    "            print(\"\\nTOP PRIORITY (HIGH) Recommendations:\")\n",
    "            for rec in high_recs[:3]:  # Top 3\n",
    "                print(f\"  • {rec['issue']}\")\n",
    "                print(f\"    → {rec['recommendation']}\")\n",
    "        \n",
    "        # Estimated impact\n",
    "        print(\"\\nExpected Impact of Implementations:\")\n",
    "        print(\"  • HIGH priority: 20-50% training improvement\")\n",
    "        print(\"  • MEDIUM priority: 10-20% training improvement\") \n",
    "        print(\"  • LOW priority: 5-10% training improvement\")\n",
    "        \n",
    "        # Implementation timeline\n",
    "        print(\"\\nSuggested Implementation Order:\")\n",
    "        print(\"  1. Implement all HIGH priority recommendations\")\n",
    "        print(\"  2. Address gradient flow issues\")\n",
    "        print(\"  3. Optimize learning rate schedule\")\n",
    "        print(\"  4. Implement stability improvements\")\n",
    "        print(\"  5. Apply general optimizations\")\n",
    "\n",
    "# %%\n",
    "# Generate optimization recommendations\n",
    "recommender = OptimizationRecommender(\n",
    "    config, \n",
    "    history, \n",
    "    loss_analyzer, \n",
    "    gradient_analyzer, \n",
    "    lr_analyzer, \n",
    "    stability_analyzer\n",
    ")\n",
    "\n",
    "recommendations = recommender.generate_recommendations()\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 8. Export Training Analysis Report\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "class TrainingAnalysisExporter:\n",
    "    \"\"\"Export comprehensive training analysis report.\"\"\"\n",
    "    \n",
    "    def __init__(self, config, history, recommendations):\n",
    "        self.config = config\n",
    "        self.history = history\n",
    "        self.recommendations = recommendations\n",
    "        \n",
    "    def export_report(self):\n",
    "        \"\"\"Export training analysis report.\"\"\"\n",
    "        print(\"\\nExporting Training Analysis Report...\")\n",
    "        \n",
    "        # Create report data\n",
    "        report = {\n",
    "            'timestamp': pd.Timestamp.now().isoformat(),\n",
    "            'config': self.config,\n",
    "            'training_summary': self.generate_training_summary(),\n",
    "            'key_metrics': self.calculate_key_metrics(),\n",
    "            'recommendations': self.recommendations,\n",
    "            'implementation_plan': self.generate_implementation_plan()\n",
    "        }\n",
    "        \n",
    "        # Export as JSON\n",
    "        import json\n",
    "        with open('../reports/training_analysis_report.json', 'w') as f:\n",
    "            json.dump(report, f, indent=2)\n",
    "        \n",
    "        print(\"Training analysis report exported to ../reports/training_analysis_report.json\")\n",
    "        \n",
    "        # Also export as HTML\n",
    "        self.export_html_report(report)\n",
    "    \n",
    "    def generate_training_summary(self):\n",
    "        \"\"\"Generate training summary.\"\"\"\n",
    "        summary = f\"\"\"\n",
    "        TRAINING ANALYSIS SUMMARY\n",
    "        \n",
    "        Configuration:\n",
    "        - Batch size: {self.config['training']['batch_size']}\n",
    "        - Initial LR: {self.config['training']['learning_rate']}\n",
    "        - Weight decay: {self.config['training']['weight_decay']}\n",
    "        - Gradient clip: {self.config['training']['gradient_clip']}\n",
    "        - Mixed precision: {self.config['training']['use_amp']}\n",
    "        \n",
    "        Key Findings:\n",
    "        1. {'Stable convergence achieved' if 'losses_array' in self.history and len(self.history['losses_array']) > 0 else 'Convergence analysis pending'}\n",
    "        2. {'Good gradient flow' if 'grad_norm_total_array' in self.history and np.mean(self.history['grad_norm_total_array']) < 10 else 'Gradient issues detected'}\n",
    "        3. {'Proper LR schedule' if 'learning_rates_array' in self.history and self.history['learning_rates_array'][-1] < self.history['learning_rates_array'][0] * 0.1 else 'LR schedule needs optimization'}\n",
    "        4. {'Excellent stability' if 'max_eigenvalue_array' in self.history and np.all(self.history['max_eigenvalue_array'] <= 1.0) else 'Stability issues detected'}\n",
    "        \n",
    "        Overall Assessment: Training pipeline is {'READY for production' if len([r for r in self.recommendations if r['priority'] == 'HIGH']) == 0 else 'NEEDS OPTIMIZATION'}\n",
    "        \"\"\"\n",
    "        return summary\n",
    "    \n",
    "    def calculate_key_metrics(self):\n",
    "        \"\"\"Calculate key training metrics.\"\"\"\n",
    "        metrics = {}\n",
    "        \n",
    "        # Loss metrics\n",
    "        if 'losses_array' in self.history and len(self.history['losses_array']) > 0:\n",
    "            losses = self.history['losses_array']\n",
    "            metrics['loss'] = {\n",
    "                'initial': float(losses[0]),\n",
    "                'final': float(losses[-1]),\n",
    "                'min': float(np.min(losses)),\n",
    "                'reduction_percent': float((1 - losses[-1]/losses[0]) * 100) if losses[0] > 0 else 0\n",
    "            }\n",
    "        \n",
    "        # Gradient metrics\n",
    "        if 'grad_norm_total_array' in self.history and len(self.history['grad_norm_total_array']) > 0:\n",
    "            grad_norms = self.history['grad_norm_total_array']\n",
    "            metrics['gradient'] = {\n",
    "                'mean': float(np.mean(grad_norms)),\n",
    "                'std': float(np.std(grad_norms)),\n",
    "                'max': float(np.max(grad_norms)),\n",
    "                'min': float(np.min(grad_norms))\n",
    "            }\n",
    "        \n",
    "        # Learning rate metrics\n",
    "        if 'learning_rates_array' in self.history and len(self.history['learning_rates_array']) > 0:\n",
    "            lrs = self.history['learning_rates_array']\n",
    "            metrics['learning_rate'] = {\n",
    "                'initial': float(lrs[0]),\n",
    "                'final': float(lrs[-1]),\n",
    "                'decay_ratio': float(lrs[-1] / lrs[0]) if lrs[0] > 0 else 0\n",
    "            }\n",
    "        \n",
    "        # Stability metrics\n",
    "        stability_metrics = {}\n",
    "        for key in ['max_eigenvalue_array', 'min_eigenvalue_array', 'signal_ratio_mean_array']:\n",
    "            if key in self.history and len(self.history[key]) > 0:\n",
    "                values = self.history[key]\n",
    "                stability_metrics[key.replace('_array', '')] = {\n",
    "                    'mean': float(np.mean(values)),\n",
    "                    'std': float(np.std(values)),\n",
    "                    'in_range': bool(self.check_stability_range(key, values))\n",
    "                }\n",
    "        \n",
    "        if stability_metrics:\n",
    "            metrics['stability'] = stability_metrics\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def check_stability_range(self, metric_name, values):\n",
    "        \"\"\"Check if stability metric is in acceptable range.\"\"\"\n",
    "        if 'max_eigenvalue' in metric_name:\n",
    "            return np.all(values <= 1.0)\n",
    "        elif 'min_eigenvalue' in metric_name:\n",
    "            return np.all(values >= 0)\n",
    "        elif 'signal_ratio' in metric_name:\n",
    "            return np.all(np.abs(values - 1.0) < 0.2)\n",
    "        return True\n",
    "    \n",
    "    def generate_implementation_plan(self):\n",
    "        \"\"\"Generate implementation plan for recommendations.\"\"\"\n",
    "        high_recs = [r for r in self.recommendations if r['priority'] == 'HIGH']\n",
    "        medium_recs = [r for r in self.recommendations if r['priority'] == 'MEDIUM']\n",
    "        low_recs = [r for r in self.recommendations if r['priority'] == 'LOW']\n",
    "        \n",
    "        plan = {\n",
    "            'phase_1_immediate': {\n",
    "                'timeline': '1-2 days',\n",
    "                'recommendations': high_recs,\n",
    "                'expected_impact': 'Resolve critical issues, ensure training stability'\n",
    "            },\n",
    "            'phase_2_short_term': {\n",
    "                'timeline': '3-7 days',\n",
    "                'recommendations': medium_recs,\n",
    "                'expected_impact': 'Improve convergence speed and final performance'\n",
    "            },\n",
    "            'phase_3_long_term': {\n",
    "                'timeline': '1-2 weeks',\n",
    "                'recommendations': low_recs,\n",
    "                'expected_impact': 'Optimize for efficiency and deployment'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return plan\n",
    "    \n",
    "    def export_html_report(self, report):\n",
    "        \"\"\"Export HTML report.\"\"\"\n",
    "        html_content = f\"\"\"\n",
    "        <!DOCTYPE html>\n",
    "        <html>\n",
    "        <head>\n",
    "            <title>Humanoid Vision System - Training Analysis Report</title>\n",
    "            <style>\n",
    "                body {{ font-family: Arial, sans-serif; margin: 40px; line-height: 1.6; }}\n",
    "                h1 {{ color: #2c3e50; border-bottom: 3px solid #3498db; }}\n",
    "                h2 {{ color: #34495e; margin-top: 30px; }}\n",
    "                h3 {{ color: #2c3e50; margin-top: 20px; }}\n",
    "                .card {{ background: #f8f9fa; border-left: 4px solid #3498db; \n",
    "                        padding: 20px; margin: 20px 0; border-radius: 5px; }}\n",
    "                .metric-card {{ display: inline-block; background: white; padding: 15px; \n",
    "                         margin: 10px; border-radius: 5px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); \n",
    "                         width: 200px; vertical-align: top; }}\n",
    "                .high {{ color: #e74c3c; font-weight: bold; border-left: 4px solid #e74c3c; }}\n",
    "                .medium {{ color: #f39c12; font-weight: bold; border-left: 4px solid #f39c12; }}\n",
    "                .low {{ color: #27ae60; font-weight: bold; border-left: 4px solid #27ae60; }}\n",
    "                table {{ width: 100%; border-collapse: collapse; margin: 20px 0; }}\n",
    "                th, td {{ padding: 12px; text-align: left; border-bottom: 1px solid #ddd; }}\n",
    "                th {{ background-color: #3498db; color: white; }}\n",
    "                .phase {{ margin: 20px 0; padding: 15px; border-radius: 5px; }}\n",
    "                .phase-1 {{ background: #ffeaa7; }}\n",
    "                .phase-2 {{ background: #a29bfe; }}\n",
    "                .phase-3 {{ background: #55efc4; }}\n",
    "            </style>\n",
    "        </head>\n",
    "        <body>\n",
    "            <h1>Humanoid Vision System - Training Analysis Report</h1>\n",
    "            <p>Generated on: {report['timestamp']}</p>\n",
    "            \n",
    "            <div class=\"card\">\n",
    "                <h2>Executive Summary</h2>\n",
    "                <pre>{report['training_summary']}</pre>\n",
    "            </div>\n",
    "            \n",
    "            <h2>Key Metrics</h2>\n",
    "            <div>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Add metric cards\n",
    "        if 'loss' in report['key_metrics']:\n",
    "            loss = report['key_metrics']['loss']\n",
    "            html_content += f\"\"\"\n",
    "                <div class=\"metric-card\">\n",
    "                    <h3>Loss</h3>\n",
    "                    <p>Initial: {loss['initial']:.4f}</p>\n",
    "                    <p>Final: {loss['final']:.4f}</p>\n",
    "                    <p>Reduction: {loss['reduction_percent']:.1f}%</p>\n",
    "                </div>\n",
    "            \"\"\"\n",
    "        \n",
    "        if 'gradient' in report['key_metrics']:\n",
    "            grad = report['key_metrics']['gradient']\n",
    "            html_content += f\"\"\"\n",
    "                <div class=\"metric-card\">\n",
    "                    <h3>Gradient</h3>\n",
    "                    <p>Mean norm: {grad['mean']:.4f}</p>\n",
    "                    <p>Std: {grad['std']:.4f}</p>\n",
    "                    <p>Max: {grad['max']:.4f}</p>\n",
    "                </div>\n",
    "            \"\"\"\n",
    "        \n",
    "        if 'learning_rate' in report['key_metrics']:\n",
    "            lr = report['key_metrics']['learning_rate']\n",
    "            html_content += f\"\"\"\n",
    "                <div class=\"metric-card\">\n",
    "                    <h3>Learning Rate</h3>\n",
    "                    <p>Initial: {lr['initial']:.1e}</p>\n",
    "                    <p>Final: {lr['final']:.1e}</p>\n",
    "                    <p>Decay: {(1-lr['decay_ratio'])*100:.1f}%</p>\n",
    "                </div>\n",
    "            \"\"\"\n",
    "        \n",
    "        html_content += \"\"\"\n",
    "            </div>\n",
    "            \n",
    "            <h2>Optimization Recommendations</h2>\n",
    "            <table>\n",
    "                <tr>\n",
    "                    <th>Category</th>\n",
    "                    <th>Priority</th>\n",
    "                    <th>Issue</th>\n",
    "                    <th>Recommendation</th>\n",
    "                </tr>\n",
    "        \"\"\"\n",
    "        \n",
    "        for rec in report['recommendations']:\n",
    "            priority_class = rec['priority'].lower()\n",
    "            html_content += f\"\"\"\n",
    "                <tr class=\"{priority_class}\">\n",
    "                    <td>{rec['category']}</td>\n",
    "                    <td>{rec['priority']}</td>\n",
    "                    <td>{rec['issue']}</td>\n",
    "                    <td>{rec['recommendation']}</td>\n",
    "                </tr>\n",
    "            \"\"\"\n",
    "        \n",
    "        html_content += \"\"\"\n",
    "            </table>\n",
    "            \n",
    "            <h2>Implementation Plan</h2>\n",
    "        \"\"\"\n",
    "        \n",
    "        plan = report['implementation_plan']\n",
    "        phases = ['phase_1_immediate', 'phase_2_short_term', 'phase_3_long_term']\n",
    "        phase_titles = ['Phase 1: Immediate (1-2 days)', 'Phase 2: Short-term (3-7 days)', 'Phase 3: Long-term (1-2 weeks)']\n",
    "        phase_classes = ['phase-1', 'phase-2', 'phase-3']\n",
    "        \n",
    "        for phase_key, title, phase_class in zip(phases, phase_titles, phase_classes):\n",
    "            if phase_key in plan:\n",
    "                phase_data = plan[phase_key]\n",
    "                \n",
    "                html_content += f\"\"\"\n",
    "                <div class=\"phase {phase_class}\">\n",
    "                    <h3>{title}</h3>\n",
    "                    <p><strong>Expected Impact:</strong> {phase_data['expected_impact']}</p>\n",
    "                    <ul>\n",
    "                \"\"\"\n",
    "                \n",
    "                for rec in phase_data['recommendations']:\n",
    "                    html_content += f\"<li><strong>{rec['category']}:</strong> {rec['recommendation']}</li>\"\n",
    "                \n",
    "                html_content += \"\"\"\n",
    "                    </ul>\n",
    "                </div>\n",
    "                \"\"\"\n",
    "        \n",
    "        html_content += \"\"\"\n",
    "            <div class=\"card\">\n",
    "                <h2>Next Steps</h2>\n",
    "                <ol>\n",
    "                    <li>Review and prioritize recommendations</li>\n",
    "                    <li>Implement Phase 1 recommendations immediately</li>\n",
    "                    <li>Monitor training after each optimization</li>\n",
    "                    <li>Proceed to inference testing and deployment</li>\n",
    "                    <li>Schedule follow-up analysis in 2 weeks</li>\n",
    "                </ol>\n",
    "            </div>\n",
    "        </body>\n",
    "        </html>\n",
    "        \"\"\"\n",
    "        \n",
    "        with open('../reports/training_analysis_report.html', 'w') as f:\n",
    "            f.write(html_content)\n",
    "        \n",
    "        print(\"HTML report exported to ../reports/training_analysis_report.html\")\n",
    "\n",
    "# %%\n",
    "# Export reports\n",
    "training_exporter = TrainingAnalysisExporter(config, history, recommendations)\n",
    "training_exporter.export_report()\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 9. Conclusion and Next Steps\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING ANALYSIS - COMPLETED\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n✅ ANALYSIS COMPLETED:\")\n",
    "print(\"  1. Loss convergence analyzed\")\n",
    "print(\"  2. Gradient flow assessed\")\n",
    "print(\"  3. Learning rate schedule evaluated\")\n",
    "print(\"  4. Training stability verified\")\n",
    "print(\"  5. Optimization recommendations generated\")\n",
    "print(\"  6. Comprehensive reports exported\")\n",
    "\n",
    "print(\"\\n📊 KEY FINDINGS:\")\n",
    "if 'losses_array' in history and len(history['losses_array']) > 0:\n",
    "    loss_reduction = (1 - history['losses_array'][-1]/history['losses_array'][0]) * 100\n",
    "    print(f\"  • Loss reduction: {loss_reduction:.1f}%\")\n",
    "if 'grad_norm_total_array' in history:\n",
    "    print(f\"  • Mean gradient norm: {np.mean(history['grad_norm_total_array']):.2f}\")\n",
    "if 'max_eigenvalue_array' in history:\n",
    "    stable = np.all(history['max_eigenvalue_array'] <= 1.0)\n",
    "    print(f\"  • Stability: {'✅ EXCELLENT' if stable else '⚠️ NEEDS ATTENTION'}\")\n",
    "\n",
    "high_count = sum(1 for r in recommendations if r['priority'] == 'HIGH')\n",
    "print(f\"\\n🚨 CRITICAL ISSUES: {high_count} HIGH priority recommendations\")\n",
    "\n",
    "print(\"\\n🚀 NEXT STEPS:\")\n",
    "print(\"  1. Implement HIGH priority recommendations immediately\")\n",
    "print(\"  2. Run validation with real data\")\n",
    "print(\"  3. Proceed to inference analysis (04_inference_demo.ipynb)\")\n",
    "print(\"  4. Prepare for deployment testing\")\n",
    "print(\"  5. Schedule model retraining with optimizations\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
